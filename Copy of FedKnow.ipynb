{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPt1TrrmYlaCwTzFxxBgfD5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbZcy3k5SnKL","executionInfo":{"status":"ok","timestamp":1676593265736,"user_tz":300,"elapsed":379903,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"3760fe6a-9a22-4b1b-8675-7a892255d04e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras==2.4.0\n","  Downloading Keras-2.4.0-py2.py3-none-any.whl (170 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.2/170.2 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.8/dist-packages (from keras==2.4.0) (1.7.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from keras==2.4.0) (6.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from keras==2.4.0) (3.1.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras==2.4.0) (1.21.6)\n","Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from keras==2.4.0) (2.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (15.0.6.1)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (23.1.21)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (57.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (23.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.14.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.15.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.6.3)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.19.6)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.2.0)\n","Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.11.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.2.0)\n","Collecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n","  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.30.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.51.1)\n","Collecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flatbuffers<2,>=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Collecting tensorboard<2.10,>=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.4.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.4.0)\n","Collecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.8.4-cp38-cp38-manylinux2010_x86_64.whl (498.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.8.3-cp38-cp38-manylinux2010_x86_64.whl (498.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.4/498.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.8.2-cp38-cp38-manylinux2010_x86_64.whl (498.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.8.1-cp38-cp38-manylinux2010_x86_64.whl (498.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.7.4-cp38-cp38-manylinux2010_x86_64.whl (496.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.0/496.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.11.2)\n","Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n","  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 KB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras-preprocessing>=1.1.1\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.7.3-cp38-cp38-manylinux2010_x86_64.whl (495.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.5/495.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.7.2-cp38-cp38-manylinux2010_x86_64.whl (495.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.5/495.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.7.1-cp38-cp38-manylinux2010_x86_64.whl (495.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.1/495.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.6/489.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.6.5-cp38-cp38-manylinux2010_x86_64.whl (464.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.3/464.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard<2.7,>=2.6.0\n","  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions<3.11,>=3.7\n","  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n","Collecting wrapt~=1.12.1\n","  Downloading wrapt-1.12.1.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting numpy>=1.9.1\n","  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.38.4)\n","Collecting clang~=5.0\n","  Downloading clang-5.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorflow-estimator<2.7,>=2.6.0\n","  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.9/462.9 KB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.6.4-cp38-cp38-manylinux2010_x86_64.whl (464.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.3/464.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.6.3-cp38-cp38-manylinux2010_x86_64.whl (463.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.9/463.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.6.2-cp38-cp38-manylinux2010_x86_64.whl (458.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.4/458.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions~=3.7.4\n","  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n","Collecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.6.1-cp38-cp38-manylinux2010_x86_64.whl (458.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.4/458.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl (458.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.4/458.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.5.3-cp38-cp38-manylinux2010_x86_64.whl (460.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.4/460.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n","  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting absl-py~=0.10\n","  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.6.0,>=2.5.0\n","  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.4/462.4 KB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting grpcio~=1.34.0\n","  Downloading grpcio-1.34.1-cp38-cp38-manylinux2014_x86_64.whl (4.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting termcolor~=1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (3.4.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (2.16.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (2.25.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (6.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (4.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (3.12.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (3.2.2)\n","Building wheels for collected packages: termcolor, wrapt\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=8f1476e531dce83ee38b8e7b21bb7f62aac04be628d9ee281d2864205270cf44\n","  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=78572 sha256=a3c49a5513d6156bd62de7523f2ccd03dd99a64261c093fc1101a1a43a72af46\n","  Stored in directory: /root/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n","Successfully built termcolor wrapt\n","Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, keras-nightly, flatbuffers, numpy, grpcio, absl-py, keras-preprocessing, tensorflow, keras\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.14.1\n","    Uninstalling wrapt-1.14.1:\n","      Successfully uninstalled wrapt-1.14.1\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.4.0\n","    Uninstalling typing_extensions-4.4.0:\n","      Successfully uninstalled typing_extensions-4.4.0\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 2.2.0\n","    Uninstalling termcolor-2.2.0:\n","      Successfully uninstalled termcolor-2.2.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.11.0\n","    Uninstalling tensorflow-estimator-2.11.0:\n","      Successfully uninstalled tensorflow-estimator-2.11.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 23.1.21\n","    Uninstalling flatbuffers-23.1.21:\n","      Successfully uninstalled flatbuffers-23.1.21\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.51.1\n","    Uninstalling grpcio-1.51.1:\n","      Successfully uninstalled grpcio-1.51.1\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 1.4.0\n","    Uninstalling absl-py-1.4.0:\n","      Successfully uninstalled absl-py-1.4.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.11.0\n","    Uninstalling tensorflow-2.11.0:\n","      Successfully uninstalled tensorflow-2.11.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.11.0\n","    Uninstalling keras-2.11.0:\n","      Successfully uninstalled keras-2.11.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","pydantic 1.10.4 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","jax 0.3.25 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.34.1 which is incompatible.\n","google-cloud-bigquery 3.4.2 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.34.1 which is incompatible.\n","cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed absl-py-0.15.0 flatbuffers-1.12 grpcio-1.34.1 keras-2.4.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 numpy-1.19.5 tensorflow-2.5.3 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"]}],"source":["pip install keras==2.4.0"]},{"cell_type":"code","source":["pip install mia"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HfJa2lN6TLeG","executionInfo":{"status":"ok","timestamp":1676594293279,"user_tz":300,"elapsed":20900,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"5ee1f5c5-bbaf-41fd-81e7-f600a594c0c0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mia\n","  Downloading mia-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mia) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from mia) (1.7.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from mia) (1.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from mia) (1.13.1+cu116)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mia) (4.64.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->mia) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->mia) (1.2.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->mia) (3.7.4.3)\n","Building wheels for collected packages: mia\n","  Building wheel for mia (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mia: filename=mia-0.1.2-py3-none-any.whl size=11106 sha256=ebfdb65b6ac0488af380408dd1d05ef444ba441d6eeeac54594c98b9868eaeec\n","  Stored in directory: /root/.cache/pip/wheels/c7/fb/77/a632189442690e610b8fad3eea1887996438e0ffc3cdadc039\n","Successfully built mia\n","Installing collected packages: mia\n","Successfully installed mia-0.1.2\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import random\n","import datetime\n","#import tensorflow_privacy\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from tensorflow import keras"],"metadata":{"id":"HJKdTaFGTIEm","executionInfo":{"status":"ok","timestamp":1676595575548,"user_tz":300,"elapsed":263,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["def load_mnist():\n","  \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n","  train, test = tf.keras.datasets.mnist.load_data()\n","  #train, test = tf.keras.datasets.fashion_mnist.load_data()\n","  #train, test = tf.keras.datasets.cifar10.load_data()\n","  train_data, train_labels = train\n","  test_data, test_labels = test\n","\n","  train_data = np.array(train_data, dtype=np.float32) / 255\n","  test_data = np.array(test_data, dtype=np.float32) / 255\n","\n","  train_data = train_data.reshape((train_data.shape[0], 28, 28, 1))\n","  test_data = test_data.reshape((test_data.shape[0], 28, 28, 1))\n","  #train_data = train_data.reshape((train_data.shape[0], 32, 32, 3))\n","  #test_data = test_data.reshape((test_data.shape[0], 32, 32, 3))\n","\n","  train_labels = np.array(train_labels, dtype=np.int32)\n","  test_labels = np.array(test_labels, dtype=np.int32)\n","\n","  #train_labels = np.array(train_labels, dtype=np.int64)\n","  #test_labels = np.array(test_labels, dtype=np.int64)\n","\n","  train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n","  test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n","\n","  #assert train_data.min() == 0.\n","  #assert train_data.max() == 1.\n","  #assert test_data.min() == 0.\n","  #assert test_data.max() == 1.\n","\n","  return train_data, train_labels, test_data, test_labels"],"metadata":{"id":"3QhVW1NHTWUj","executionInfo":{"status":"ok","timestamp":1676595577619,"user_tz":300,"elapsed":303,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["train_data, train_labels, test_data, test_labels = load_mnist()"],"metadata":{"id":"Ikl8SMpdTZUi","executionInfo":{"status":"ok","timestamp":1676595581550,"user_tz":300,"elapsed":658,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["cifar_train = train_data, train_labels\n","cifar_test = test_data, test_labels"],"metadata":{"id":"ZyGRFXFITeNB","executionInfo":{"status":"ok","timestamp":1676595583033,"user_tz":300,"elapsed":281,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["CLIENTS = 3\n","SIZE = 10000\n","\n","def get_data(source):   \n","    \n","    all_data = (np.array(source[0][:SIZE*CLIENTS]), source[1][:SIZE*CLIENTS]) \n","    \n","    split_data = []\n","    for s in range(CLIENTS):\n","        start = s*SIZE\n","        end = s*SIZE + SIZE\n","        split_data.append((all_data[0][start:end], all_data[1][start:end]))\n","    \n","    external_data = (np.array(source[0][SIZE*CLIENTS:]), source[1][SIZE*CLIENTS:]) \n","    \n","    return all_data, split_data, external_data"],"metadata":{"id":"LMGF0SKoThFF","executionInfo":{"status":"ok","timestamp":1676595584634,"user_tz":300,"elapsed":253,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["CLIENTS = 3\n","SIZE = 1000\n","\n","def get_test_data(source):   \n","    \n","    all_data = (np.array(source[0][:SIZE*CLIENTS]), source[1][:SIZE*CLIENTS]) \n","    \n","    split_data = []\n","    for s in range(CLIENTS):\n","        start = s*SIZE\n","        end = s*SIZE + SIZE\n","        split_data.append((all_data[0][start:end], all_data[1][start:end]))\n","    \n","    external_data = (np.array(source[0][SIZE*CLIENTS:]), source[1][SIZE*CLIENTS:]) \n","    \n","    return all_data, split_data, external_data"],"metadata":{"id":"djQtTiFSTjW-","executionInfo":{"status":"ok","timestamp":1676595587067,"user_tz":300,"elapsed":307,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["cifar_train_data, cifar_train_fed_data, attacker_data = get_data(cifar_train)"],"metadata":{"id":"ID6r68WyTl2B","executionInfo":{"status":"ok","timestamp":1676595589507,"user_tz":300,"elapsed":278,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["cifar_test_data, cifar_test_fed_data, externat_test_data = get_test_data(cifar_test)"],"metadata":{"id":"2K72iAIfTop3","executionInfo":{"status":"ok","timestamp":1676595591026,"user_tz":300,"elapsed":278,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["def create_compiled_keras_model():\n","    model = tf.keras.models.Sequential([\n","        tf.keras.layers.Flatten(input_shape=[28, 28, 1]),\n","        tf.keras.layers.Dense(300, activation=\"relu\"),\n","        tf.keras.layers.Dense(100, activation=\"relu\"),\n","        tf.keras.layers.Dense(10, activation=\"softmax\")\n","    ])\n","    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","    model.compile(loss=loss,\n","              optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01),\n","              metrics=[\"accuracy\"])\n","    return model"],"metadata":{"id":"DnUsOG3Ei3k-","executionInfo":{"status":"ok","timestamp":1676595696512,"user_tz":300,"elapsed":782,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["teacher_model = create_compiled_keras_model()"],"metadata":{"id":"L4i6inN5utfK","executionInfo":{"status":"ok","timestamp":1676595786116,"user_tz":300,"elapsed":273,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["student_model = create_compiled_keras_model()"],"metadata":{"id":"jOy8IjWGVXCl","executionInfo":{"status":"ok","timestamp":1676595789508,"user_tz":300,"elapsed":277,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["class Distiller(keras.Model):\n","    def __init__(self, student, teacher):\n","        super().__init__()\n","        self.teacher = teacher\n","        self.student = student\n","\n","    def compile(\n","        self,\n","        optimizer,\n","        metrics,\n","        student_loss_fn,\n","        distillation_loss_fn,\n","        alpha=0.1,\n","        temperature=3,\n","    ):\n","        \"\"\" Configure the distiller.\n","\n","        Args:\n","            optimizer: Keras optimizer for the student weights\n","            metrics: Keras metrics for evaluation\n","            student_loss_fn: Loss function of difference between student\n","                predictions and ground-truth\n","            distillation_loss_fn: Loss function of difference between soft\n","                student predictions and soft teacher predictions\n","            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n","            temperature: Temperature for softening probability distributions.\n","                Larger temperature gives softer distributions.\n","        \"\"\"\n","        super().compile(optimizer=optimizer, metrics=metrics)\n","        self.student_loss_fn = student_loss_fn\n","        self.distillation_loss_fn = distillation_loss_fn\n","        self.alpha = alpha\n","        self.temperature = temperature\n","\n","    def train_step(self, data):\n","        # Unpack data\n","        x, y = data\n","\n","        # Forward pass of teacher\n","        teacher_predictions = self.teacher(x, training=False)\n","\n","        with tf.GradientTape() as tape:\n","            # Forward pass of student\n","            student_predictions = self.student(x, training=True)\n","\n","            # Compute losses\n","            student_loss = self.student_loss_fn(y, student_predictions)\n","\n","            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n","            # The magnitudes of the gradients produced by the soft targets scale\n","            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n","            distillation_loss = (\n","                self.distillation_loss_fn(\n","                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n","                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n","                )\n","                * self.temperature**2\n","            )\n","\n","            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n","\n","        # Compute gradients\n","        trainable_vars = self.student.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # Update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # Update the metrics configured in `compile()`.\n","        self.compiled_metrics.update_state(y, student_predictions)\n","\n","        # Return a dict of performance\n","        results = {m.name: m.result() for m in self.metrics}\n","        results.update(\n","            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n","        )\n","        return results\n","\n","    def test_step(self, data):\n","        # Unpack the data\n","        x, y = data\n","\n","        # Compute predictions\n","        y_prediction = self.student(x, training=False)\n","\n","        # Calculate the loss\n","        student_loss = self.student_loss_fn(y, y_prediction)\n","\n","        # Update the metrics.\n","        self.compiled_metrics.update_state(y, y_prediction)\n","\n","        # Return a dict of performance\n","        results = {m.name: m.result() for m in self.metrics}\n","        results.update({\"student_loss\": student_loss})\n","        return results"],"metadata":{"id":"LiocM3jmXFFC","executionInfo":{"status":"ok","timestamp":1676595790764,"user_tz":300,"elapsed":308,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["initial_model = create_compiled_keras_model()\n","\n","FedTrain = []\n","FedTest = []\n","FedAcc = []\n","\n","DistTrain = []\n","DistTest = []\n","DistAcc = []\n","\n","Dstudent_loss = []\n","Ddistillation_loss = []\n","Daccuracy = []\n","\n","for r in range(18):\n","    \n","    deltas = []\n","\n","    for c in range(CLIENTS):\n","\n","        teacher_model = create_compiled_keras_model()\n","\n","        teacher_model.set_weights(initial_model.get_weights())\n","\n","        #fed_history_callback = federated_model.fit(cifar_train_fed_data[c][0], cifar_train_fed_data[c][1], \n","         #                                      batch_size=250, epochs=10, verbose=1)\n","        fed_history_callback = teacher_model.fit(cifar_train_fed_data[c][0], cifar_train_fed_data[c][1], \n","                                               batch_size=32, epochs=10, verbose=1)\n","        \n","        delta = np.array(initial_model.get_weights()) - np.array(teacher_model.get_weights())\n","\n","        deltas.append(delta)\n","\n","    print('Epoch {}/18'.format(r+1))\n","    delt_av = (deltas[0] + deltas[1] + deltas[2]) / 3\n","    new_weights = np.array(initial_model.get_weights()) - delt_av\n","    initial_model.set_weights(new_weights)\n","    \n","    FedTrain.append(initial_model.evaluate(cifar_train_data[0], cifar_train_data[1])[0])\n","    validation = initial_model.evaluate(cifar_test_data[0], cifar_test_data[1])\n","    FedTest.append(validation[0])\n","    FedAcc.append(validation[1])\n","\n","    distiller = Distiller(student=student_model, teacher=teacher_model)\n","    distiller.compile(\n","    optimizer=keras.optimizers.SGD(),\n","    metrics=[\"accuracy\"],\n","    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n","    distillation_loss_fn=keras.losses.KLDivergence(),\n","    alpha=0.1,\n","    temperature=10,\n","    )\n","    # Distill teacher to student\n","    dist = distiller.fit(cifar_train_data[0], cifar_train_data[1], epochs=10)\n","    Dstudent_loss.append(dist.history['student_loss'])\n","    Ddistillation_loss.append(dist.history['distillation_loss'])\n","    Daccuracy.append(dist.history['accuracy'])\n","\n","    # Evaluate student on test dataset\n","    #distiller.evaluate(cifar_test_data[0], cifar_test_data[1])\n","\n","    DistTrain.append(distiller.evaluate(cifar_train_data[0], cifar_train_data[1])[0])\n","    validation = distiller.evaluate(cifar_test_data[0], cifar_test_data[1])\n","    DistTest.append(validation[0])\n","    DistAcc.append(validation[1])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cu6-C26KT3K2","executionInfo":{"status":"ok","timestamp":1676596198768,"user_tz":300,"elapsed":404942,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"7bb78484-a1e9-470b-e5ba-e26302021a29"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 1.9280 - accuracy: 0.5970\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.5169 - accuracy: 0.8450\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.3320 - accuracy: 0.8840\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2185 - accuracy: 0.9320\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2766 - accuracy: 0.9390\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1302 - accuracy: 0.9600\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0647 - accuracy: 0.9830\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1293 - accuracy: 0.9650\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0774 - accuracy: 0.9760\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0374 - accuracy: 0.9900\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-80-7054eab812ff>:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  delta = np.array(initial_model.get_weights()) - np.array(teacher_model.get_weights())\n"]},{"output_type":"stream","name":"stdout","text":["32/32 [==============================] - 1s 5ms/step - loss: 2.3516 - accuracy: 0.5520\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.6304 - accuracy: 0.8150\n","Epoch 3/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.4381 - accuracy: 0.8740\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2973 - accuracy: 0.9160\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2105 - accuracy: 0.9400\n","Epoch 6/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.2012 - accuracy: 0.9570\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1164 - accuracy: 0.9700\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1873 - accuracy: 0.9630\n","Epoch 9/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9590\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0858 - accuracy: 0.9770\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 2.3167 - accuracy: 0.5970\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.5161 - accuracy: 0.8420\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.3651 - accuracy: 0.8790\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2055 - accuracy: 0.9400\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.1957 - accuracy: 0.9360\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0951 - accuracy: 0.9770\n","Epoch 7/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1306 - accuracy: 0.9730\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1296 - accuracy: 0.9660\n","Epoch 9/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1471 - accuracy: 0.9810\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0756 - accuracy: 0.9870\n","Epoch 1/18\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-80-7054eab812ff>:36: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  new_weights = np.array(initial_model.get_weights()) - delt_av\n"]},{"output_type":"stream","name":"stdout","text":["94/94 [==============================] - 0s 3ms/step - loss: 0.5160 - accuracy: 0.8883\n","94/94 [==============================] - 0s 3ms/step - loss: 0.6788 - accuracy: 0.8180\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/backend.py:4869: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["94/94 [==============================] - 1s 8ms/step - accuracy: 0.1550 - student_loss: 2.2818 - distillation_loss: 0.0457\n","Epoch 2/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.2307 - student_loss: 2.2009 - distillation_loss: 0.0448\n","Epoch 3/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.3293 - student_loss: 2.1266 - distillation_loss: 0.0439\n","Epoch 4/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.4057 - student_loss: 2.0492 - distillation_loss: 0.0428\n","Epoch 5/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.4753 - student_loss: 1.9724 - distillation_loss: 0.0417\n","Epoch 6/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.5370 - student_loss: 1.8966 - distillation_loss: 0.0404\n","Epoch 7/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.5757 - student_loss: 1.8177 - distillation_loss: 0.0391\n","Epoch 8/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.6073 - student_loss: 1.7433 - distillation_loss: 0.0377\n","Epoch 9/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.6413 - student_loss: 1.6648 - distillation_loss: 0.0363\n","Epoch 10/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.6663 - student_loss: 1.5852 - distillation_loss: 0.0347\n","94/94 [==============================] - 1s 4ms/step - accuracy: 0.6773 - student_loss: 1.5470\n","94/94 [==============================] - 0s 2ms/step - accuracy: 0.5930 - student_loss: 1.6860\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.8935 - accuracy: 0.8690\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2187 - accuracy: 0.9420\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2137 - accuracy: 0.9450\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1809 - accuracy: 0.9600\n","Epoch 5/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1866 - accuracy: 0.9590\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1591 - accuracy: 0.9630\n","Epoch 7/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9770\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1295 - accuracy: 0.9730\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1115 - accuracy: 0.9780\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0521 - accuracy: 0.9930\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.9491 - accuracy: 0.8290\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2700 - accuracy: 0.9240\n","Epoch 3/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.2256 - accuracy: 0.9380\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1901 - accuracy: 0.9600\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.1747 - accuracy: 0.9640\n","Epoch 6/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1684 - accuracy: 0.9610\n","Epoch 7/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1691 - accuracy: 0.9640\n","Epoch 8/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9730\n","Epoch 9/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9890\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1367 - accuracy: 0.9750\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 1.2150 - accuracy: 0.8330\n","Epoch 2/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1921 - accuracy: 0.9500\n","Epoch 3/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1919 - accuracy: 0.9470\n","Epoch 4/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1563 - accuracy: 0.9590\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1426 - accuracy: 0.9670\n","Epoch 6/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.1725 - accuracy: 0.9740\n","Epoch 7/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.0845 - accuracy: 0.9820\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1376 - accuracy: 0.9830\n","Epoch 9/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.0900 - accuracy: 0.9830\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0875 - accuracy: 0.9840\n","Epoch 2/18\n","94/94 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.9650\n","94/94 [==============================] - 0s 3ms/step - loss: 0.6454 - accuracy: 0.8967\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.6863 - student_loss: 1.5159 - distillation_loss: 0.0334\n","Epoch 2/10\n","94/94 [==============================] - 1s 7ms/step - accuracy: 0.7067 - student_loss: 1.4392 - distillation_loss: 0.0318\n","Epoch 3/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.7217 - student_loss: 1.3686 - distillation_loss: 0.0303\n","Epoch 4/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.7400 - student_loss: 1.3008 - distillation_loss: 0.0288\n","Epoch 5/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.7573 - student_loss: 1.2329 - distillation_loss: 0.0273\n","Epoch 6/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.7720 - student_loss: 1.1774 - distillation_loss: 0.0261\n","Epoch 7/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.7863 - student_loss: 1.1191 - distillation_loss: 0.0248\n","Epoch 8/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.7993 - student_loss: 1.0684 - distillation_loss: 0.0237\n","Epoch 9/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.8067 - student_loss: 1.0235 - distillation_loss: 0.0226\n","Epoch 10/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.8127 - student_loss: 0.9738 - distillation_loss: 0.0215\n","94/94 [==============================] - 0s 2ms/step - accuracy: 0.8173 - student_loss: 0.9510\n","94/94 [==============================] - 0s 2ms/step - accuracy: 0.7343 - student_loss: 1.1384\n","Epoch 1/10\n","32/32 [==============================] - 34s 5ms/step - loss: 0.5692 - accuracy: 0.9270\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0839 - accuracy: 0.9760\n","Epoch 3/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.2170 - accuracy: 0.9580\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0688 - accuracy: 0.9760\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0847 - accuracy: 0.9830\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0778 - accuracy: 0.9890\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0354 - accuracy: 0.9910\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0742 - accuracy: 0.9840\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1119 - accuracy: 0.9840\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0695 - accuracy: 0.9910\n","Epoch 1/10\n","32/32 [==============================] - 1s 8ms/step - loss: 0.6388 - accuracy: 0.9160\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1367 - accuracy: 0.9610\n","Epoch 3/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.2168 - accuracy: 0.9580\n","Epoch 4/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.1046 - accuracy: 0.9750\n","Epoch 5/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1701 - accuracy: 0.9750\n","Epoch 6/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1003 - accuracy: 0.9770\n","Epoch 7/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0555 - accuracy: 0.9850\n","Epoch 8/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1438 - accuracy: 0.9830\n","Epoch 9/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1120 - accuracy: 0.9840\n","Epoch 10/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0511 - accuracy: 0.9940\n","Epoch 1/10\n","32/32 [==============================] - 1s 9ms/step - loss: 0.6937 - accuracy: 0.9280\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1197 - accuracy: 0.9700\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1982 - accuracy: 0.9590\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0845 - accuracy: 0.9830\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2711 - accuracy: 0.9640\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1127 - accuracy: 0.9830\n","Epoch 7/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.0501 - accuracy: 0.9960\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1180 - accuracy: 0.9840\n","Epoch 9/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.2137 - accuracy: 0.9680\n","Epoch 10/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9880\n","Epoch 3/18\n","94/94 [==============================] - 0s 3ms/step - loss: 0.0898 - accuracy: 0.9793\n","94/94 [==============================] - 0s 2ms/step - loss: 0.7962 - accuracy: 0.9197\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.8180 - student_loss: 0.9320 - distillation_loss: 0.0207\n","Epoch 2/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8260 - student_loss: 0.8950 - distillation_loss: 0.0198\n","Epoch 3/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8297 - student_loss: 0.8635 - distillation_loss: 0.0191\n","Epoch 4/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8333 - student_loss: 0.8301 - distillation_loss: 0.0183\n","Epoch 5/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8377 - student_loss: 0.8012 - distillation_loss: 0.0176\n","Epoch 6/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8420 - student_loss: 0.7745 - distillation_loss: 0.0169\n","Epoch 7/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8453 - student_loss: 0.7467 - distillation_loss: 0.0163\n","Epoch 8/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8497 - student_loss: 0.7255 - distillation_loss: 0.0158\n","Epoch 9/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8520 - student_loss: 0.7052 - distillation_loss: 0.0154\n","Epoch 10/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8560 - student_loss: 0.6884 - distillation_loss: 0.0150\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.8573 - student_loss: 0.6749\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.7830 - student_loss: 0.8618\n","Epoch 1/10\n","32/32 [==============================] - 1s 8ms/step - loss: 0.4992 - accuracy: 0.9420\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.1515 - accuracy: 0.9780\n","Epoch 3/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0368 - accuracy: 0.9910\n","Epoch 4/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1568 - accuracy: 0.9790\n","Epoch 5/10\n","32/32 [==============================] - 0s 12ms/step - loss: 0.0952 - accuracy: 0.9820\n","Epoch 6/10\n","32/32 [==============================] - 0s 12ms/step - loss: 0.0567 - accuracy: 0.9900\n","Epoch 7/10\n","32/32 [==============================] - 1s 17ms/step - loss: 2.7223e-04 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 1s 21ms/step - loss: 8.7399e-06 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 13ms/step - loss: 1.6487e-06 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 13ms/step - loss: 0.1001 - accuracy: 0.9930\n","Epoch 1/10\n","32/32 [==============================] - 2s 13ms/step - loss: 0.3802 - accuracy: 0.9460\n","Epoch 2/10\n","32/32 [==============================] - 0s 13ms/step - loss: 0.1567 - accuracy: 0.9790\n","Epoch 3/10\n","32/32 [==============================] - 0s 13ms/step - loss: 0.2050 - accuracy: 0.9730\n","Epoch 4/10\n","32/32 [==============================] - 1s 16ms/step - loss: 0.1777 - accuracy: 0.9800\n","Epoch 5/10\n","32/32 [==============================] - 1s 19ms/step - loss: 0.0925 - accuracy: 0.9820\n","Epoch 6/10\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1530 - accuracy: 0.9800\n","Epoch 7/10\n","32/32 [==============================] - 0s 13ms/step - loss: 0.0481 - accuracy: 0.9910\n","Epoch 8/10\n","32/32 [==============================] - 0s 13ms/step - loss: 0.1353 - accuracy: 0.9860\n","Epoch 9/10\n","32/32 [==============================] - 0s 12ms/step - loss: 0.0787 - accuracy: 0.9840\n","Epoch 10/10\n","32/32 [==============================] - 0s 11ms/step - loss: 0.0980 - accuracy: 0.9910\n","Epoch 1/10\n","32/32 [==============================] - 1s 10ms/step - loss: 0.4580 - accuracy: 0.9560\n","Epoch 2/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0882 - accuracy: 0.9890\n","Epoch 3/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1008 - accuracy: 0.9850\n","Epoch 4/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1881 - accuracy: 0.9760\n","Epoch 5/10\n","32/32 [==============================] - 0s 11ms/step - loss: 0.2588 - accuracy: 0.9720\n","Epoch 6/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.1070 - accuracy: 0.9870\n","Epoch 7/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.1033 - accuracy: 0.9860\n","Epoch 8/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0731 - accuracy: 0.9930\n","Epoch 9/10\n","32/32 [==============================] - 0s 11ms/step - loss: 0.2164 - accuracy: 0.9780\n","Epoch 10/10\n","32/32 [==============================] - 0s 11ms/step - loss: 0.0885 - accuracy: 0.9890\n","Epoch 4/18\n","94/94 [==============================] - 0s 4ms/step - loss: 0.0662 - accuracy: 0.9903\n","94/94 [==============================] - 0s 3ms/step - loss: 1.0886 - accuracy: 0.9197\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.8583 - student_loss: 0.6696 - distillation_loss: 0.0143\n","Epoch 2/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8587 - student_loss: 0.6512 - distillation_loss: 0.0138\n","Epoch 3/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8603 - student_loss: 0.6357 - distillation_loss: 0.0135\n","Epoch 4/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8633 - student_loss: 0.6196 - distillation_loss: 0.0131\n","Epoch 5/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8647 - student_loss: 0.6075 - distillation_loss: 0.0128\n","Epoch 6/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8663 - student_loss: 0.5926 - distillation_loss: 0.0125\n","Epoch 7/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8670 - student_loss: 0.5839 - distillation_loss: 0.0123\n","Epoch 8/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8693 - student_loss: 0.5702 - distillation_loss: 0.0120\n","Epoch 9/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8703 - student_loss: 0.5599 - distillation_loss: 0.0117\n","Epoch 10/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8717 - student_loss: 0.5552 - distillation_loss: 0.0115\n","94/94 [==============================] - 0s 2ms/step - accuracy: 0.8733 - student_loss: 0.5418\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.8067 - student_loss: 0.7272\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.4400 - accuracy: 0.9610\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1190 - accuracy: 0.9740\n","Epoch 3/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.0713 - accuracy: 0.9900\n","Epoch 4/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.1369 - accuracy: 0.9840\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0202 - accuracy: 0.9960\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2316 - accuracy: 0.9870\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0624 - accuracy: 0.9930\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.9221e-04 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1254e-05 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.1040e-07 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 8ms/step - loss: 0.4433 - accuracy: 0.9630\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.2487 - accuracy: 0.9720\n","Epoch 3/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1325 - accuracy: 0.9830\n","Epoch 4/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0094 - accuracy: 0.9960\n","Epoch 5/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0891 - accuracy: 0.9890\n","Epoch 6/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1218 - accuracy: 0.9820\n","Epoch 7/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0165 - accuracy: 0.9970\n","Epoch 8/10\n","32/32 [==============================] - 0s 9ms/step - loss: 6.5599e-04 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 9ms/step - loss: 2.7008e-06 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 10ms/step - loss: 2.0134e-07 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 9ms/step - loss: 0.2576 - accuracy: 0.9680\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0997 - accuracy: 0.9910\n","Epoch 3/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0538 - accuracy: 0.9930\n","Epoch 4/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.1910 - accuracy: 0.9820\n","Epoch 5/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0540 - accuracy: 0.9930\n","Epoch 6/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.1184 - accuracy: 0.9900\n","Epoch 7/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1034 - accuracy: 0.9870\n","Epoch 8/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0284 - accuracy: 0.9960\n","Epoch 9/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0662 - accuracy: 0.9900\n","Epoch 10/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0762 - accuracy: 0.9950\n","Epoch 5/18\n","94/94 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9953\n","94/94 [==============================] - 0s 3ms/step - loss: 1.0451 - accuracy: 0.9253\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.8747 - student_loss: 0.5458 - distillation_loss: 0.0118\n","Epoch 2/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8743 - student_loss: 0.5360 - distillation_loss: 0.0115\n","Epoch 3/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.8763 - student_loss: 0.5192 - distillation_loss: 0.0112\n","Epoch 4/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8763 - student_loss: 0.5126 - distillation_loss: 0.0111\n","Epoch 5/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8803 - student_loss: 0.5035 - distillation_loss: 0.0108\n","Epoch 6/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8803 - student_loss: 0.5005 - distillation_loss: 0.0108\n","Epoch 7/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8817 - student_loss: 0.4880 - distillation_loss: 0.0105\n","Epoch 8/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8820 - student_loss: 0.4843 - distillation_loss: 0.0104\n","Epoch 9/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8830 - student_loss: 0.4761 - distillation_loss: 0.0102\n","Epoch 10/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8843 - student_loss: 0.4696 - distillation_loss: 0.0101\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.8853 - student_loss: 0.4646\n","94/94 [==============================] - 0s 2ms/step - accuracy: 0.8253 - student_loss: 0.6485\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.2234 - accuracy: 0.9750\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0180 - accuracy: 0.9960\n","Epoch 3/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0908 - accuracy: 0.9890\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.9970\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0274 - accuracy: 0.9970\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0138 - accuracy: 0.9990\n","Epoch 7/10\n","32/32 [==============================] - 0s 9ms/step - loss: 4.8995e-08 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 10ms/step - loss: 2.1815e-08 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 9ms/step - loss: 7.9870e-09 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 9ms/step - loss: 3.8147e-09 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 8ms/step - loss: 0.2866 - accuracy: 0.9660\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0247 - accuracy: 0.9910\n","Epoch 3/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0839 - accuracy: 0.9870\n","Epoch 4/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.1289 - accuracy: 0.9820\n","Epoch 5/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.1368 - accuracy: 0.9850\n","Epoch 6/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0621 - accuracy: 0.9890\n","Epoch 7/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0045 - accuracy: 0.9990\n","Epoch 8/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0204 - accuracy: 0.9970\n","Epoch 9/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0473 - accuracy: 0.9960\n","Epoch 10/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1256 - accuracy: 0.9890\n","Epoch 1/10\n","32/32 [==============================] - 1s 5ms/step - loss: 0.2758 - accuracy: 0.9730\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1409 - accuracy: 0.9850\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2037 - accuracy: 0.9820\n","Epoch 4/10\n","32/32 [==============================] - 0s 5ms/step - loss: 0.2205 - accuracy: 0.9790\n","Epoch 5/10\n","32/32 [==============================] - 0s 5ms/step - loss: 8.6195e-06 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 7.3227e-07 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 2.5201e-07 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.3089e-07 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 4.4703e-08 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1802e-08 - accuracy: 1.0000\n","Epoch 6/18\n","94/94 [==============================] - 0s 3ms/step - loss: 0.0146 - accuracy: 0.9963\n","94/94 [==============================] - 0s 3ms/step - loss: 1.2253 - accuracy: 0.9250\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.8850 - student_loss: 0.4643 - distillation_loss: 0.0100\n","Epoch 2/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8867 - student_loss: 0.4571 - distillation_loss: 0.0098\n","Epoch 3/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8880 - student_loss: 0.4540 - distillation_loss: 0.0097\n","Epoch 4/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8893 - student_loss: 0.4468 - distillation_loss: 0.0096\n","Epoch 5/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8910 - student_loss: 0.4402 - distillation_loss: 0.0094\n","Epoch 6/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8900 - student_loss: 0.4368 - distillation_loss: 0.0093\n","Epoch 7/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8913 - student_loss: 0.4344 - distillation_loss: 0.0093\n","Epoch 8/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8943 - student_loss: 0.4278 - distillation_loss: 0.0091\n","Epoch 9/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8930 - student_loss: 0.4235 - distillation_loss: 0.0090\n","Epoch 10/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.8930 - student_loss: 0.4179 - distillation_loss: 0.0089\n","94/94 [==============================] - 0s 2ms/step - accuracy: 0.8953 - student_loss: 0.4133\n","94/94 [==============================] - 0s 2ms/step - accuracy: 0.8370 - student_loss: 0.5965\n","Epoch 1/10\n","32/32 [==============================] - 1s 9ms/step - loss: 0.2089 - accuracy: 0.9790\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1755 - accuracy: 0.9820\n","Epoch 3/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0832 - accuracy: 0.9910\n","Epoch 4/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0341 - accuracy: 0.9960\n","Epoch 5/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0613 - accuracy: 0.9920\n","Epoch 6/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0061 - accuracy: 0.9980\n","Epoch 7/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0316 - accuracy: 0.9940\n","Epoch 8/10\n","32/32 [==============================] - 0s 9ms/step - loss: 1.7263e-05 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0732 - accuracy: 0.9950\n","Epoch 10/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.1275 - accuracy: 0.9910\n","Epoch 1/10\n","32/32 [==============================] - 1s 10ms/step - loss: 0.2806 - accuracy: 0.9790\n","Epoch 2/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.1293 - accuracy: 0.9910\n","Epoch 3/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 0.9990\n","Epoch 4/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0410 - accuracy: 0.9970\n","Epoch 5/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0921 - accuracy: 0.9920\n","Epoch 6/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1770 - accuracy: 0.9840\n","Epoch 7/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0340 - accuracy: 0.9950\n","Epoch 8/10\n","32/32 [==============================] - 0s 9ms/step - loss: 5.6619e-04 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1868 - accuracy: 0.9850\n","Epoch 10/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0645 - accuracy: 0.9900\n","Epoch 1/10\n","32/32 [==============================] - 2s 6ms/step - loss: 0.2477 - accuracy: 0.9690\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0666 - accuracy: 0.9930\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0679 - accuracy: 0.9950\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.8589e-05 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 4.2677e-08 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.6703e-08 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.3590e-08 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 6.3181e-09 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 2.2650e-09 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 8.3446e-10 - accuracy: 1.0000\n","Epoch 7/18\n","94/94 [==============================] - 0s 3ms/step - loss: 0.0070 - accuracy: 0.9987\n","94/94 [==============================] - 0s 3ms/step - loss: 1.4003 - accuracy: 0.9277\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.8953 - student_loss: 0.4118 - distillation_loss: 0.0090\n","Epoch 2/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.8957 - student_loss: 0.4101 - distillation_loss: 0.0089\n","Epoch 3/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.8987 - student_loss: 0.4066 - distillation_loss: 0.0088\n","Epoch 4/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9000 - student_loss: 0.4006 - distillation_loss: 0.0087\n","Epoch 5/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9010 - student_loss: 0.3988 - distillation_loss: 0.0087\n","Epoch 6/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9007 - student_loss: 0.3961 - distillation_loss: 0.0086\n","Epoch 7/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9020 - student_loss: 0.3906 - distillation_loss: 0.0085\n","Epoch 8/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9033 - student_loss: 0.3850 - distillation_loss: 0.0083\n","Epoch 9/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9033 - student_loss: 0.3859 - distillation_loss: 0.0083\n","Epoch 10/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9047 - student_loss: 0.3797 - distillation_loss: 0.0082\n","94/94 [==============================] - 0s 2ms/step - accuracy: 0.9053 - student_loss: 0.3764\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.8433 - student_loss: 0.5600\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.2713 - accuracy: 0.9820\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0585 - accuracy: 0.9900\n","Epoch 3/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1081 - accuracy: 0.9910\n","Epoch 4/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0342 - accuracy: 0.9970\n","Epoch 5/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0488 - accuracy: 0.9960\n","Epoch 6/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0221 - accuracy: 0.9990\n","Epoch 7/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.3428 - accuracy: 0.9760\n","Epoch 8/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0116 - accuracy: 0.9980\n","Epoch 9/10\n","32/32 [==============================] - 0s 9ms/step - loss: 1.1846e-06 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 9ms/step - loss: 1.8001e-08 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 9ms/step - loss: 0.1839 - accuracy: 0.9870\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1752 - accuracy: 0.9890\n","Epoch 3/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.2369 - accuracy: 0.9870\n","Epoch 4/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0729 - accuracy: 0.9950\n","Epoch 5/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0956 - accuracy: 0.9890\n","Epoch 6/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1360 - accuracy: 0.9930\n","Epoch 7/10\n","32/32 [==============================] - 0s 10ms/step - loss: 2.2696e-07 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 9ms/step - loss: 4.0173e-08 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 10ms/step - loss: 2.0266e-08 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 10ms/step - loss: 1.0490e-08 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.2695 - accuracy: 0.9840\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1543 - accuracy: 0.9810\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0893 - accuracy: 0.9940\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.2022 - accuracy: 0.9860\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0500 - accuracy: 0.9930\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.8510e-05 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0305 - accuracy: 0.9980\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 5.2452e-09 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.5034e-09 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.5497e-09 - accuracy: 1.0000\n","Epoch 8/18\n","94/94 [==============================] - 0s 3ms/step - loss: 0.0050 - accuracy: 0.9990\n","94/94 [==============================] - 0s 3ms/step - loss: 1.6935 - accuracy: 0.9277\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9053 - student_loss: 0.3784 - distillation_loss: 0.0080\n","Epoch 2/10\n","94/94 [==============================] - 1s 6ms/step - accuracy: 0.9070 - student_loss: 0.3725 - distillation_loss: 0.0079\n","Epoch 3/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9047 - student_loss: 0.3713 - distillation_loss: 0.0079\n","Epoch 4/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9067 - student_loss: 0.3683 - distillation_loss: 0.0079\n","Epoch 5/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9077 - student_loss: 0.3661 - distillation_loss: 0.0078\n","Epoch 6/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9073 - student_loss: 0.3627 - distillation_loss: 0.0077\n","Epoch 7/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9083 - student_loss: 0.3567 - distillation_loss: 0.0076\n","Epoch 8/10\n","94/94 [==============================] - 1s 6ms/step - accuracy: 0.9083 - student_loss: 0.3550 - distillation_loss: 0.0075\n","Epoch 9/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9100 - student_loss: 0.3550 - distillation_loss: 0.0075\n","Epoch 10/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9110 - student_loss: 0.3523 - distillation_loss: 0.0075\n","94/94 [==============================] - 0s 4ms/step - accuracy: 0.9117 - student_loss: 0.3479\n","94/94 [==============================] - 0s 4ms/step - accuracy: 0.8483 - student_loss: 0.5334\n","Epoch 1/10\n","32/32 [==============================] - 1s 9ms/step - loss: 0.0910 - accuracy: 0.9930\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0344 - accuracy: 0.9950\n","Epoch 3/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.1530 - accuracy: 0.9880\n","Epoch 4/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0889 - accuracy: 0.9920\n","Epoch 5/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0325 - accuracy: 0.9960\n","Epoch 6/10\n","32/32 [==============================] - 0s 9ms/step - loss: 6.0588e-06 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 8ms/step - loss: 2.3842e-09 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 9ms/step - loss: 1.9073e-09 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 9ms/step - loss: 1.3113e-09 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 9ms/step - loss: 1.0729e-09 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.1983 - accuracy: 0.9840\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0908 - accuracy: 0.9960\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0169 - accuracy: 0.9970\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 0.9940\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0931 - accuracy: 0.9910\n","Epoch 6/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0895 - accuracy: 0.9890\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 5.5187e-05 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.8085e-07 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.3554e-07 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 9.7868e-08 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0129 - accuracy: 0.9970\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.2770 - accuracy: 0.9810\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0469 - accuracy: 0.9940\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1232 - accuracy: 0.9910\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0434 - accuracy: 0.9970\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 7.3748e-05 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.3351e-08 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 5.4836e-09 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.2650e-09 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 9.5367e-10 - accuracy: 1.0000\n","Epoch 9/18\n","94/94 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9993\n","94/94 [==============================] - 0s 3ms/step - loss: 1.8637 - accuracy: 0.9200\n","Epoch 1/10\n","94/94 [==============================] - 1s 6ms/step - accuracy: 0.9110 - student_loss: 0.3517 - distillation_loss: 0.0075\n","Epoch 2/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9113 - student_loss: 0.3444 - distillation_loss: 0.0074\n","Epoch 3/10\n","94/94 [==============================] - 1s 6ms/step - accuracy: 0.9120 - student_loss: 0.3451 - distillation_loss: 0.0074\n","Epoch 4/10\n","94/94 [==============================] - 0s 5ms/step - accuracy: 0.9133 - student_loss: 0.3433 - distillation_loss: 0.0073\n","Epoch 5/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9123 - student_loss: 0.3416 - distillation_loss: 0.0073\n","Epoch 6/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9157 - student_loss: 0.3361 - distillation_loss: 0.0072\n","Epoch 7/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9153 - student_loss: 0.3356 - distillation_loss: 0.0072\n","Epoch 8/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9157 - student_loss: 0.3338 - distillation_loss: 0.0071\n","Epoch 9/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9167 - student_loss: 0.3304 - distillation_loss: 0.0070\n","Epoch 10/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9180 - student_loss: 0.3274 - distillation_loss: 0.0070\n","94/94 [==============================] - 1s 4ms/step - accuracy: 0.9187 - student_loss: 0.3248\n","94/94 [==============================] - 0s 4ms/step - accuracy: 0.8533 - student_loss: 0.5135\n","Epoch 1/10\n","32/32 [==============================] - 1s 10ms/step - loss: 0.0525 - accuracy: 0.9920\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0473 - accuracy: 0.9960\n","Epoch 3/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0679 - accuracy: 0.9980\n","Epoch 4/10\n","32/32 [==============================] - 0s 10ms/step - loss: 1.7664e-06 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 9ms/step - loss: 7.1526e-10 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 9ms/step - loss: 3.5763e-10 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 9ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 9ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.1833 - accuracy: 0.9880\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - accuracy: 0.9960\n","Epoch 3/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0212 - accuracy: 0.9960\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.1757e-05 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 6.2280e-07 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.0265e-08 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 4.1723e-09 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 2.5034e-09 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.4305e-09 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 7ms/step - loss: 7.1526e-10 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.3199 - accuracy: 0.9840\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0622 - accuracy: 0.9950\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0450 - accuracy: 0.9970\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.8575e-06 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.4305e-09 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1921e-09 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.0729e-09 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 8.3446e-10 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 3.5763e-10 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 10/18\n","94/94 [==============================] - 0s 3ms/step - loss: 0.0037 - accuracy: 0.9993\n","94/94 [==============================] - 0s 3ms/step - loss: 2.0860 - accuracy: 0.9250\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9183 - student_loss: 0.3241 - distillation_loss: 0.0070\n","Epoch 2/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9197 - student_loss: 0.3226 - distillation_loss: 0.0070\n","Epoch 3/10\n","94/94 [==============================] - 1s 6ms/step - accuracy: 0.9193 - student_loss: 0.3224 - distillation_loss: 0.0070\n","Epoch 4/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9203 - student_loss: 0.3187 - distillation_loss: 0.0069\n","Epoch 5/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9203 - student_loss: 0.3196 - distillation_loss: 0.0069\n","Epoch 6/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9220 - student_loss: 0.3151 - distillation_loss: 0.0068\n","Epoch 7/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9230 - student_loss: 0.3140 - distillation_loss: 0.0068\n","Epoch 8/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9227 - student_loss: 0.3162 - distillation_loss: 0.0068\n","Epoch 9/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9227 - student_loss: 0.3089 - distillation_loss: 0.0067\n","Epoch 10/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9243 - student_loss: 0.3072 - distillation_loss: 0.0066\n","94/94 [==============================] - 1s 4ms/step - accuracy: 0.9240 - student_loss: 0.3053\n","94/94 [==============================] - 0s 4ms/step - accuracy: 0.8587 - student_loss: 0.4953\n","Epoch 1/10\n","32/32 [==============================] - 1s 10ms/step - loss: 0.0592 - accuracy: 0.9950\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0827 - accuracy: 0.9940\n","Epoch 3/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0678 - accuracy: 0.9950\n","Epoch 4/10\n","32/32 [==============================] - 0s 10ms/step - loss: 1.0965e-04 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 8ms/step - loss: 4.1047e-07 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 7.1526e-10 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 5.9605e-10 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 4.7684e-10 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.3842e-10 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.2285 - accuracy: 0.9860\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0818 - accuracy: 0.9910\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0939 - accuracy: 0.9920\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0508 - accuracy: 0.9950\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.9970\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 0.9960\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0537 - accuracy: 0.9960\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 1.5425e-07 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 9.0833e-08 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.9073e-08 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0973 - accuracy: 0.9920\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1931 - accuracy: 0.9940\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.6226e-09 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.7881e-09 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1921e-09 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 7.1526e-10 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 4.7684e-10 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 2.3842e-10 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 11/18\n","94/94 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 0.9997\n","94/94 [==============================] - 0s 3ms/step - loss: 2.1627 - accuracy: 0.9210\n","Epoch 1/10\n","94/94 [==============================] - 2s 6ms/step - accuracy: 0.9230 - student_loss: 0.3070 - distillation_loss: 0.0067\n","Epoch 2/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9237 - student_loss: 0.3030 - distillation_loss: 0.0065\n","Epoch 3/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9247 - student_loss: 0.3035 - distillation_loss: 0.0066\n","Epoch 4/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9260 - student_loss: 0.3017 - distillation_loss: 0.0065\n","Epoch 5/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9257 - student_loss: 0.3015 - distillation_loss: 0.0065\n","Epoch 6/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9267 - student_loss: 0.3030 - distillation_loss: 0.0065\n","Epoch 7/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9270 - student_loss: 0.2958 - distillation_loss: 0.0064\n","Epoch 8/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9287 - student_loss: 0.2953 - distillation_loss: 0.0064\n","Epoch 9/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9287 - student_loss: 0.2940 - distillation_loss: 0.0063\n","Epoch 10/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9280 - student_loss: 0.2922 - distillation_loss: 0.0063\n","94/94 [==============================] - 1s 4ms/step - accuracy: 0.9300 - student_loss: 0.2884\n","94/94 [==============================] - 0s 4ms/step - accuracy: 0.8620 - student_loss: 0.4816\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0149 - accuracy: 0.9970\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.1473 - accuracy: 0.9890\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0047 - accuracy: 0.9990\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0865 - accuracy: 0.9940\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0542 - accuracy: 0.9940\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0304 - accuracy: 0.9980\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 5.5431e-08 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 4.7684e-10 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 3.5763e-10 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.3842e-10 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0531 - accuracy: 0.9950\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0394 - accuracy: 0.9980\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0732 - accuracy: 0.9920\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.4222 - accuracy: 0.9860\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0560 - accuracy: 0.9970\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 5.9521e-07 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 6.6757e-09 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 5.3644e-09 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 3.8147e-09 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.6689e-09 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 7ms/step - loss: 3.2781e-06 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.5905e-04 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.1728 - accuracy: 0.9850\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0976 - accuracy: 0.9930\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0893 - accuracy: 0.9930\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0018 - accuracy: 0.9990\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 7.4187e-04 - accuracy: 0.9990\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0799 - accuracy: 0.9940\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0218 - accuracy: 0.9990\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0905 - accuracy: 0.9930\n","Epoch 12/18\n","94/94 [==============================] - 0s 3ms/step - loss: 2.3023e-06 - accuracy: 1.0000\n","94/94 [==============================] - 0s 3ms/step - loss: 2.3325 - accuracy: 0.9273\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9283 - student_loss: 0.2919 - distillation_loss: 0.0063\n","Epoch 2/10\n","94/94 [==============================] - 1s 7ms/step - accuracy: 0.9287 - student_loss: 0.2911 - distillation_loss: 0.0064\n","Epoch 3/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9293 - student_loss: 0.2868 - distillation_loss: 0.0062\n","Epoch 4/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9303 - student_loss: 0.2846 - distillation_loss: 0.0062\n","Epoch 5/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9280 - student_loss: 0.2823 - distillation_loss: 0.0061\n","Epoch 6/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9300 - student_loss: 0.2834 - distillation_loss: 0.0062\n","Epoch 7/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9300 - student_loss: 0.2799 - distillation_loss: 0.0061\n","Epoch 8/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9297 - student_loss: 0.2796 - distillation_loss: 0.0061\n","Epoch 9/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9303 - student_loss: 0.2767 - distillation_loss: 0.0060\n","Epoch 10/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9300 - student_loss: 0.2756 - distillation_loss: 0.0060\n","94/94 [==============================] - 1s 4ms/step - accuracy: 0.9310 - student_loss: 0.2739\n","94/94 [==============================] - 0s 4ms/step - accuracy: 0.8650 - student_loss: 0.4691\n","Epoch 1/10\n","32/32 [==============================] - 1s 10ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0054 - accuracy: 0.9990\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0457 - accuracy: 0.9970\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0290 - accuracy: 0.9980\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0392 - accuracy: 0.9940\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 0.9990\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.3815e-07 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.6822e-08 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 2.3842e-10 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 13/18\n","94/94 [==============================] - 0s 3ms/step - loss: 1.1535e-06 - accuracy: 1.0000\n","94/94 [==============================] - 0s 3ms/step - loss: 2.4264 - accuracy: 0.9293\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9303 - student_loss: 0.2746 - distillation_loss: 0.0061\n","Epoch 2/10\n","94/94 [==============================] - 1s 7ms/step - accuracy: 0.9317 - student_loss: 0.2751 - distillation_loss: 0.0062\n","Epoch 3/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9323 - student_loss: 0.2714 - distillation_loss: 0.0061\n","Epoch 4/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9317 - student_loss: 0.2717 - distillation_loss: 0.0061\n","Epoch 5/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9320 - student_loss: 0.2696 - distillation_loss: 0.0060\n","Epoch 6/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9327 - student_loss: 0.2692 - distillation_loss: 0.0060\n","Epoch 7/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9320 - student_loss: 0.2656 - distillation_loss: 0.0059\n","Epoch 8/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9330 - student_loss: 0.2671 - distillation_loss: 0.0059\n","Epoch 9/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9330 - student_loss: 0.2640 - distillation_loss: 0.0059\n","Epoch 10/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9337 - student_loss: 0.2631 - distillation_loss: 0.0059\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.9340 - student_loss: 0.2605\n","94/94 [==============================] - 0s 2ms/step - accuracy: 0.8690 - student_loss: 0.4589\n","Epoch 1/10\n","32/32 [==============================] - 1s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 3.7430e-06 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0243 - accuracy: 0.9970\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0539 - accuracy: 0.9950\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0726 - accuracy: 0.9930\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0177 - accuracy: 0.9970\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0071 - accuracy: 0.9990\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0200 - accuracy: 0.9960\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0068 - accuracy: 0.9990\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 5.3644e-09 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 14/18\n","94/94 [==============================] - 0s 4ms/step - loss: 6.7974e-04 - accuracy: 0.9997\n","94/94 [==============================] - 0s 4ms/step - loss: 2.4704 - accuracy: 0.9273\n","Epoch 1/10\n","94/94 [==============================] - 2s 9ms/step - accuracy: 0.9323 - student_loss: 0.2609 - distillation_loss: 0.0058\n","Epoch 2/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9357 - student_loss: 0.2652 - distillation_loss: 0.0059\n","Epoch 3/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9340 - student_loss: 0.2576 - distillation_loss: 0.0057\n","Epoch 4/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9340 - student_loss: 0.2594 - distillation_loss: 0.0058\n","Epoch 5/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9347 - student_loss: 0.2590 - distillation_loss: 0.0058\n","Epoch 6/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9360 - student_loss: 0.2540 - distillation_loss: 0.0057\n","Epoch 7/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9353 - student_loss: 0.2559 - distillation_loss: 0.0057\n","Epoch 8/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9357 - student_loss: 0.2540 - distillation_loss: 0.0057\n","Epoch 9/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9347 - student_loss: 0.2541 - distillation_loss: 0.0057\n","Epoch 10/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9353 - student_loss: 0.2519 - distillation_loss: 0.0056\n","94/94 [==============================] - 1s 4ms/step - accuracy: 0.9363 - student_loss: 0.2486\n","94/94 [==============================] - 0s 4ms/step - accuracy: 0.8713 - student_loss: 0.4488\n","Epoch 1/10\n","32/32 [==============================] - 1s 10ms/step - loss: 7.7486e-09 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 4.9865e-07 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 2.7418e-09 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 15/18\n","94/94 [==============================] - 0s 3ms/step - loss: 2.5983e-06 - accuracy: 1.0000\n","94/94 [==============================] - 0s 3ms/step - loss: 2.4970 - accuracy: 0.9277\n","Epoch 1/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9357 - student_loss: 0.2559 - distillation_loss: 0.0056\n","Epoch 2/10\n","94/94 [==============================] - 1s 5ms/step - accuracy: 0.9350 - student_loss: 0.2514 - distillation_loss: 0.0056\n","Epoch 3/10\n","94/94 [==============================] - 1s 6ms/step - accuracy: 0.9363 - student_loss: 0.2489 - distillation_loss: 0.0055\n","Epoch 4/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9380 - student_loss: 0.2473 - distillation_loss: 0.0055\n","Epoch 5/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9373 - student_loss: 0.2461 - distillation_loss: 0.0054\n","Epoch 6/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9370 - student_loss: 0.2481 - distillation_loss: 0.0055\n","Epoch 7/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9393 - student_loss: 0.2462 - distillation_loss: 0.0055\n","Epoch 8/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9383 - student_loss: 0.2442 - distillation_loss: 0.0054\n","Epoch 9/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9380 - student_loss: 0.2400 - distillation_loss: 0.0053\n","Epoch 10/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9380 - student_loss: 0.2433 - distillation_loss: 0.0054\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.9387 - student_loss: 0.2378\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.8723 - student_loss: 0.4409\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 1.4305e-09 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0866 - accuracy: 0.9960\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 8.7780e-06 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 7ms/step - loss: 2.3842e-10 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 16/18\n","94/94 [==============================] - 0s 4ms/step - loss: 3.9736e-11 - accuracy: 1.0000\n","94/94 [==============================] - 0s 4ms/step - loss: 2.4954 - accuracy: 0.9260\n","Epoch 1/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9390 - student_loss: 0.2386 - distillation_loss: 0.0053\n","Epoch 2/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9397 - student_loss: 0.2374 - distillation_loss: 0.0053\n","Epoch 3/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9407 - student_loss: 0.2375 - distillation_loss: 0.0053\n","Epoch 4/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9397 - student_loss: 0.2362 - distillation_loss: 0.0053\n","Epoch 5/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9400 - student_loss: 0.2346 - distillation_loss: 0.0052\n","Epoch 6/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9403 - student_loss: 0.2379 - distillation_loss: 0.0053\n","Epoch 7/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9410 - student_loss: 0.2322 - distillation_loss: 0.0051\n","Epoch 8/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9403 - student_loss: 0.2315 - distillation_loss: 0.0051\n","Epoch 9/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9420 - student_loss: 0.2301 - distillation_loss: 0.0051\n","Epoch 10/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9407 - student_loss: 0.2288 - distillation_loss: 0.0051\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.9430 - student_loss: 0.2278\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.8750 - student_loss: 0.4344\n","Epoch 1/10\n","32/32 [==============================] - 1s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 7ms/step - loss: 1.1921e-10 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 17/18\n","94/94 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","94/94 [==============================] - 0s 4ms/step - loss: 2.4999 - accuracy: 0.9260\n","Epoch 1/10\n","94/94 [==============================] - 2s 9ms/step - accuracy: 0.9423 - student_loss: 0.2290 - distillation_loss: 0.0051\n","Epoch 2/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9420 - student_loss: 0.2274 - distillation_loss: 0.0050\n","Epoch 3/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9417 - student_loss: 0.2287 - distillation_loss: 0.0051\n","Epoch 4/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9427 - student_loss: 0.2256 - distillation_loss: 0.0050\n","Epoch 5/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9423 - student_loss: 0.2259 - distillation_loss: 0.0050\n","Epoch 6/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9437 - student_loss: 0.2242 - distillation_loss: 0.0050\n","Epoch 7/10\n","94/94 [==============================] - 1s 10ms/step - accuracy: 0.9427 - student_loss: 0.2236 - distillation_loss: 0.0050\n","Epoch 8/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9420 - student_loss: 0.2227 - distillation_loss: 0.0049\n","Epoch 9/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9447 - student_loss: 0.2227 - distillation_loss: 0.0049\n","Epoch 10/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9433 - student_loss: 0.2233 - distillation_loss: 0.0050\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.9453 - student_loss: 0.2187\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.8760 - student_loss: 0.4273\n","Epoch 1/10\n","32/32 [==============================] - 1s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 1/10\n","32/32 [==============================] - 1s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 2/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 3/10\n","32/32 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 4/10\n","32/32 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 5/10\n","32/32 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 6/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 7/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 8/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 9/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 10/10\n","32/32 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","Epoch 18/18\n","94/94 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n","94/94 [==============================] - 0s 4ms/step - loss: 2.5039 - accuracy: 0.9257\n","Epoch 1/10\n","94/94 [==============================] - 2s 8ms/step - accuracy: 0.9440 - student_loss: 0.2212 - distillation_loss: 0.0049\n","Epoch 2/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9453 - student_loss: 0.2181 - distillation_loss: 0.0048\n","Epoch 3/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9443 - student_loss: 0.2172 - distillation_loss: 0.0048\n","Epoch 4/10\n","94/94 [==============================] - 1s 10ms/step - accuracy: 0.9443 - student_loss: 0.2175 - distillation_loss: 0.0048\n","Epoch 5/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9447 - student_loss: 0.2184 - distillation_loss: 0.0048\n","Epoch 6/10\n","94/94 [==============================] - 1s 9ms/step - accuracy: 0.9460 - student_loss: 0.2149 - distillation_loss: 0.0047\n","Epoch 7/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9460 - student_loss: 0.2134 - distillation_loss: 0.0047\n","Epoch 8/10\n","94/94 [==============================] - 1s 8ms/step - accuracy: 0.9473 - student_loss: 0.2134 - distillation_loss: 0.0047\n","Epoch 9/10\n","94/94 [==============================] - 1s 7ms/step - accuracy: 0.9460 - student_loss: 0.2126 - distillation_loss: 0.0047\n","Epoch 10/10\n","94/94 [==============================] - 1s 6ms/step - accuracy: 0.9467 - student_loss: 0.2111 - distillation_loss: 0.0047\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.9477 - student_loss: 0.2100\n","94/94 [==============================] - 0s 3ms/step - accuracy: 0.8780 - student_loss: 0.4227\n"]}]},{"cell_type":"code","source":["np.mean(dist.history['student_loss'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzVnDuoM4RJX","executionInfo":{"status":"ok","timestamp":1676597480392,"user_tz":300,"elapsed":346,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"bbf0640f-38b2-4b91-eae2-4d81f88a4ee6"},"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.15730745494365692"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["np.mean(dist.history['distillation_loss'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5YuP44qo4ene","executionInfo":{"status":"ok","timestamp":1676597482027,"user_tz":300,"elapsed":321,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"98b26a17-707d-4bd3-f2e9-e27888eba5da"},"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.003269837237894535"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["np.mean(dist.history['accuracy'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plPmEDu44kAH","executionInfo":{"status":"ok","timestamp":1676597484104,"user_tz":300,"elapsed":286,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"9e8b3137-d4fd-4fb8-adef-b813d36b257d"},"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9454666614532471"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["np.mean(DistTrain)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VxBaJRU2IzS3","executionInfo":{"status":"ok","timestamp":1676597487571,"user_tz":300,"elapsed":279,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"e410e224-ea5e-43f7-925c-ed6bd3b3ea13"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8984259366989136"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["np.mean(DistTest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KjOKi7tGI37x","executionInfo":{"status":"ok","timestamp":1676579248479,"user_tz":300,"elapsed":134,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"ca04cee9-93c6-42a8-bbe8-e4af9f286d19"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8268518480989668"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["np.mean(DistAcc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJWN23I4I7WS","executionInfo":{"status":"ok","timestamp":1676579229036,"user_tz":300,"elapsed":149,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"70191a52-f79f-4ff0-c9af-31f50a566944"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6823559204737345"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["np.mean(Dstudent_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T5Zb_rtOMrYK","executionInfo":{"status":"ok","timestamp":1676579251570,"user_tz":300,"elapsed":143,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"824cc59c-ebbe-4821-d59c-e1d45e07079e"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5154598756382863"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["np.mean(Ddistillation_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikscaG6cM4u3","executionInfo":{"status":"ok","timestamp":1676579252681,"user_tz":300,"elapsed":134,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"1ebca8ac-7883-4244-f3a1-315b44ffb54d"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.003998374488340535"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["np.mean(Daccuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eX2Ad01wM646","executionInfo":{"status":"ok","timestamp":1676597490628,"user_tz":300,"elapsed":267,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"171bc8fa-6c87-4ca2-9538-333419ffe4e2"},"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8792629619439443"]},"metadata":{},"execution_count":85}]},{"cell_type":"markdown","source":["**Membership Inference Attack**"],"metadata":{"id":"S0HNte6tB0n8"}},{"cell_type":"code","source":["import numpy as np\n","\n","from absl import app\n","from absl import flags\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data"],"metadata":{"id":"BLr06eg5B6Iu","executionInfo":{"status":"ok","timestamp":1676597503439,"user_tz":300,"elapsed":279,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["NUM_CLASSES = 10\n","\n","SHADOW_DATASET_SIZE = 1000\n","ATTACK_TEST_DATASET_SIZE = 5000\n","\n","num_shadows = 10\n","#num_shadows = 1\n"],"metadata":{"id":"qCuz1sy5B92l","executionInfo":{"status":"ok","timestamp":1676597505429,"user_tz":300,"elapsed":413,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["def target_model_fn():\n","    model = tf.keras.models.Sequential([\n","        tf.keras.layers.Flatten(input_shape=[28, 28, 1]),\n","        tf.keras.layers.Dense(300, activation=\"relu\"),\n","        tf.keras.layers.Dense(100, activation=\"relu\"),\n","        tf.keras.layers.Dense(10, activation=\"softmax\")\n","    ])\n","    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","    model.compile(loss=loss,\n","              optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01),\n","              metrics=[\"accuracy\"])\n","    return model"],"metadata":{"id":"CQIbzsh-CABC","executionInfo":{"status":"ok","timestamp":1676597507237,"user_tz":300,"elapsed":302,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["def attack_model_fn():\n","    model = tf.keras.models.Sequential()\n","\n","    model.add(layers.Dense(64, activation=\"relu\", input_shape=(10,)))\n","    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n","    model.add(layers.Dense(32, activation=\"relu\"))\n","    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n","    model.add(layers.Dense(32, activation=\"relu\"))\n","    model.add(layers.Dense(1, activation=\"sigmoid\"))\n","    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","    return model"],"metadata":{"id":"DXTQv6EKCIAo","executionInfo":{"status":"ok","timestamp":1676597509058,"user_tz":300,"elapsed":290,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["target_model = student_model"],"metadata":{"id":"O9kqSXn3CN1F","executionInfo":{"status":"ok","timestamp":1676597511423,"user_tz":300,"elapsed":303,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["# Train the shadow models.\n","smb = ShadowModelBundle(\n","    target_model_fn,\n","    shadow_dataset_size=SHADOW_DATASET_SIZE,\n","    num_models=num_shadows\n",")\n","\n","# Using cifar10 test set to train shadow models\n","attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n","    cifar_test[0], cifar_test[1], test_size=0.5)\n","\n","print(attacker_X_train.shape, attacker_X_test.shape)\n","\n","print(\"Training the shadow models...\")\n","X_shadow, y_shadow = smb.fit_transform(\n","    attacker_X_train,\n","    attacker_y_train,\n","    fit_kwargs=dict(\n","        epochs=32,\n","        verbose=True,\n","        validation_data=(attacker_X_test, attacker_y_test)\n","    )\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"513UB7iCCR9f","executionInfo":{"status":"ok","timestamp":1676597767585,"user_tz":300,"elapsed":254689,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"c22a4e23-1a55-432a-9597-e71c3525bee5"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["(5000, 28, 28, 1) (5000, 28, 28, 1)\n","Training the shadow models...\n","Epoch 1/32\n","32/32 [==============================] - 1s 20ms/step - loss: 1.9971 - accuracy: 0.4290 - val_loss: 1.6095 - val_accuracy: 0.6096\n","Epoch 2/32\n","32/32 [==============================] - 0s 14ms/step - loss: 1.2636 - accuracy: 0.7470 - val_loss: 1.0540 - val_accuracy: 0.7392\n","Epoch 3/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.8543 - accuracy: 0.8010 - val_loss: 0.7983 - val_accuracy: 0.7848\n","Epoch 4/32\n","32/32 [==============================] - 1s 26ms/step - loss: 0.6583 - accuracy: 0.8500 - val_loss: 0.6866 - val_accuracy: 0.8134\n","Epoch 5/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.5583 - accuracy: 0.8720 - val_loss: 0.5719 - val_accuracy: 0.8462\n","Epoch 6/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.4833 - accuracy: 0.8840 - val_loss: 0.5261 - val_accuracy: 0.8616\n","Epoch 7/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.4314 - accuracy: 0.8910 - val_loss: 0.4917 - val_accuracy: 0.8654\n","Epoch 8/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.3893 - accuracy: 0.9030 - val_loss: 0.4700 - val_accuracy: 0.8644\n","Epoch 9/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.3582 - accuracy: 0.9120 - val_loss: 0.4547 - val_accuracy: 0.8738\n","Epoch 10/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.3273 - accuracy: 0.9210 - val_loss: 0.4351 - val_accuracy: 0.8758\n","Epoch 11/32\n","32/32 [==============================] - 1s 27ms/step - loss: 0.3019 - accuracy: 0.9260 - val_loss: 0.4226 - val_accuracy: 0.8816\n","Epoch 12/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.2781 - accuracy: 0.9300 - val_loss: 0.3982 - val_accuracy: 0.8884\n","Epoch 13/32\n","32/32 [==============================] - 1s 18ms/step - loss: 0.2591 - accuracy: 0.9410 - val_loss: 0.4122 - val_accuracy: 0.8744\n","Epoch 14/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.2410 - accuracy: 0.9440 - val_loss: 0.4173 - val_accuracy: 0.8744\n","Epoch 15/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.2241 - accuracy: 0.9530 - val_loss: 0.4054 - val_accuracy: 0.8784\n","Epoch 16/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.2104 - accuracy: 0.9540 - val_loss: 0.3847 - val_accuracy: 0.8834\n","Epoch 17/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1956 - accuracy: 0.9560 - val_loss: 0.3936 - val_accuracy: 0.8838\n","Epoch 18/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1864 - accuracy: 0.9610 - val_loss: 0.3803 - val_accuracy: 0.8868\n","Epoch 19/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1740 - accuracy: 0.9710 - val_loss: 0.3668 - val_accuracy: 0.8896\n","Epoch 20/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1676 - accuracy: 0.9670 - val_loss: 0.3638 - val_accuracy: 0.8910\n","Epoch 21/32\n","32/32 [==============================] - 1s 26ms/step - loss: 0.1560 - accuracy: 0.9740 - val_loss: 0.3696 - val_accuracy: 0.8890\n","Epoch 22/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1453 - accuracy: 0.9750 - val_loss: 0.3627 - val_accuracy: 0.8892\n","Epoch 23/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1373 - accuracy: 0.9800 - val_loss: 0.3569 - val_accuracy: 0.8914\n","Epoch 24/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.1291 - accuracy: 0.9800 - val_loss: 0.3602 - val_accuracy: 0.8936\n","Epoch 25/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1230 - accuracy: 0.9870 - val_loss: 0.3525 - val_accuracy: 0.8928\n","Epoch 26/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1158 - accuracy: 0.9860 - val_loss: 0.3709 - val_accuracy: 0.8832\n","Epoch 27/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1122 - accuracy: 0.9860 - val_loss: 0.3546 - val_accuracy: 0.8932\n","Epoch 28/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1059 - accuracy: 0.9870 - val_loss: 0.3489 - val_accuracy: 0.8936\n","Epoch 29/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.0999 - accuracy: 0.9870 - val_loss: 0.3516 - val_accuracy: 0.8948\n","Epoch 30/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.0958 - accuracy: 0.9910 - val_loss: 0.3522 - val_accuracy: 0.8908\n","Epoch 31/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.0924 - accuracy: 0.9910 - val_loss: 0.3602 - val_accuracy: 0.8924\n","Epoch 32/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.0862 - accuracy: 0.9920 - val_loss: 0.3488 - val_accuracy: 0.8962\n","Epoch 1/32\n","32/32 [==============================] - 2s 35ms/step - loss: 2.0309 - accuracy: 0.3610 - val_loss: 1.6823 - val_accuracy: 0.6504\n","Epoch 2/32\n","32/32 [==============================] - 1s 27ms/step - loss: 1.3728 - accuracy: 0.7380 - val_loss: 1.1075 - val_accuracy: 0.7710\n","Epoch 3/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.9308 - accuracy: 0.8060 - val_loss: 0.8619 - val_accuracy: 0.7766\n","Epoch 4/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.7140 - accuracy: 0.8380 - val_loss: 0.6930 - val_accuracy: 0.8170\n","Epoch 5/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.5945 - accuracy: 0.8520 - val_loss: 0.6161 - val_accuracy: 0.8142\n","Epoch 6/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.5195 - accuracy: 0.8620 - val_loss: 0.5644 - val_accuracy: 0.8464\n","Epoch 7/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.4627 - accuracy: 0.8790 - val_loss: 0.5005 - val_accuracy: 0.8650\n","Epoch 8/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.4199 - accuracy: 0.8900 - val_loss: 0.4929 - val_accuracy: 0.8572\n","Epoch 9/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.3839 - accuracy: 0.9020 - val_loss: 0.4532 - val_accuracy: 0.8700\n","Epoch 10/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.3514 - accuracy: 0.9090 - val_loss: 0.4498 - val_accuracy: 0.8718\n","Epoch 11/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.3218 - accuracy: 0.9270 - val_loss: 0.4248 - val_accuracy: 0.8786\n","Epoch 12/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.3060 - accuracy: 0.9180 - val_loss: 0.4079 - val_accuracy: 0.8864\n","Epoch 13/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.2870 - accuracy: 0.9270 - val_loss: 0.3908 - val_accuracy: 0.8870\n","Epoch 14/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.2631 - accuracy: 0.9380 - val_loss: 0.3978 - val_accuracy: 0.8758\n","Epoch 15/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.2499 - accuracy: 0.9380 - val_loss: 0.4116 - val_accuracy: 0.8770\n","Epoch 16/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.2382 - accuracy: 0.9470 - val_loss: 0.3863 - val_accuracy: 0.8852\n","Epoch 17/32\n","32/32 [==============================] - 1s 16ms/step - loss: 0.2244 - accuracy: 0.9510 - val_loss: 0.3685 - val_accuracy: 0.8898\n","Epoch 18/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.2098 - accuracy: 0.9530 - val_loss: 0.3572 - val_accuracy: 0.8904\n","Epoch 19/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1973 - accuracy: 0.9530 - val_loss: 0.3596 - val_accuracy: 0.8924\n","Epoch 20/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1866 - accuracy: 0.9590 - val_loss: 0.3582 - val_accuracy: 0.8938\n","Epoch 21/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1761 - accuracy: 0.9660 - val_loss: 0.3469 - val_accuracy: 0.8958\n","Epoch 22/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1651 - accuracy: 0.9680 - val_loss: 0.3735 - val_accuracy: 0.8872\n","Epoch 23/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1595 - accuracy: 0.9650 - val_loss: 0.3459 - val_accuracy: 0.8960\n","Epoch 24/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.1518 - accuracy: 0.9730 - val_loss: 0.3958 - val_accuracy: 0.8744\n","Epoch 25/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.1480 - accuracy: 0.9680 - val_loss: 0.3449 - val_accuracy: 0.8942\n","Epoch 26/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.1346 - accuracy: 0.9740 - val_loss: 0.3353 - val_accuracy: 0.9002\n","Epoch 27/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.1270 - accuracy: 0.9760 - val_loss: 0.3309 - val_accuracy: 0.9006\n","Epoch 28/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.1184 - accuracy: 0.9860 - val_loss: 0.3348 - val_accuracy: 0.8980\n","Epoch 29/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.1154 - accuracy: 0.9810 - val_loss: 0.3291 - val_accuracy: 0.9000\n","Epoch 30/32\n","32/32 [==============================] - 1s 22ms/step - loss: 0.1089 - accuracy: 0.9850 - val_loss: 0.3382 - val_accuracy: 0.8948\n","Epoch 31/32\n","32/32 [==============================] - 1s 22ms/step - loss: 0.1048 - accuracy: 0.9870 - val_loss: 0.3249 - val_accuracy: 0.9024\n","Epoch 32/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0983 - accuracy: 0.9890 - val_loss: 0.3274 - val_accuracy: 0.8998\n","Epoch 1/32\n","32/32 [==============================] - 1s 29ms/step - loss: 2.0125 - accuracy: 0.4110 - val_loss: 1.6921 - val_accuracy: 0.6090\n","Epoch 2/32\n","32/32 [==============================] - 0s 14ms/step - loss: 1.3520 - accuracy: 0.7340 - val_loss: 1.1113 - val_accuracy: 0.7946\n","Epoch 3/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.8995 - accuracy: 0.8170 - val_loss: 0.8118 - val_accuracy: 0.8346\n","Epoch 4/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.6800 - accuracy: 0.8420 - val_loss: 0.6774 - val_accuracy: 0.8418\n","Epoch 5/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.5571 - accuracy: 0.8780 - val_loss: 0.5856 - val_accuracy: 0.8506\n","Epoch 6/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.4834 - accuracy: 0.8750 - val_loss: 0.5180 - val_accuracy: 0.8730\n","Epoch 7/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.4273 - accuracy: 0.8900 - val_loss: 0.5215 - val_accuracy: 0.8612\n","Epoch 8/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.3863 - accuracy: 0.9030 - val_loss: 0.4984 - val_accuracy: 0.8578\n","Epoch 9/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.3531 - accuracy: 0.9050 - val_loss: 0.4262 - val_accuracy: 0.8902\n","Epoch 10/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.3191 - accuracy: 0.9160 - val_loss: 0.4122 - val_accuracy: 0.8890\n","Epoch 11/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.2941 - accuracy: 0.9220 - val_loss: 0.4090 - val_accuracy: 0.8868\n","Epoch 12/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.2731 - accuracy: 0.9340 - val_loss: 0.3943 - val_accuracy: 0.8888\n","Epoch 13/32\n","32/32 [==============================] - 1s 27ms/step - loss: 0.2500 - accuracy: 0.9420 - val_loss: 0.3891 - val_accuracy: 0.8900\n","Epoch 14/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.2328 - accuracy: 0.9420 - val_loss: 0.3747 - val_accuracy: 0.8954\n","Epoch 15/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.2181 - accuracy: 0.9470 - val_loss: 0.3814 - val_accuracy: 0.8862\n","Epoch 16/32\n","32/32 [==============================] - 1s 27ms/step - loss: 0.2041 - accuracy: 0.9550 - val_loss: 0.3946 - val_accuracy: 0.8848\n","Epoch 17/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.1953 - accuracy: 0.9570 - val_loss: 0.3594 - val_accuracy: 0.8962\n","Epoch 18/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.1800 - accuracy: 0.9630 - val_loss: 0.3534 - val_accuracy: 0.8976\n","Epoch 19/32\n","32/32 [==============================] - 1s 21ms/step - loss: 0.1659 - accuracy: 0.9740 - val_loss: 0.3629 - val_accuracy: 0.8980\n","Epoch 20/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1596 - accuracy: 0.9690 - val_loss: 0.3617 - val_accuracy: 0.8930\n","Epoch 21/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1465 - accuracy: 0.9780 - val_loss: 0.3652 - val_accuracy: 0.8916\n","Epoch 22/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1393 - accuracy: 0.9770 - val_loss: 0.3420 - val_accuracy: 0.9002\n","Epoch 23/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1295 - accuracy: 0.9800 - val_loss: 0.3430 - val_accuracy: 0.8996\n","Epoch 24/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1224 - accuracy: 0.9830 - val_loss: 0.3719 - val_accuracy: 0.8928\n","Epoch 25/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1192 - accuracy: 0.9790 - val_loss: 0.3373 - val_accuracy: 0.9008\n","Epoch 26/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1121 - accuracy: 0.9900 - val_loss: 0.3446 - val_accuracy: 0.8972\n","Epoch 27/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1059 - accuracy: 0.9860 - val_loss: 0.3372 - val_accuracy: 0.9048\n","Epoch 28/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.0977 - accuracy: 0.9890 - val_loss: 0.3365 - val_accuracy: 0.9008\n","Epoch 29/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0929 - accuracy: 0.9920 - val_loss: 0.3374 - val_accuracy: 0.9004\n","Epoch 30/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.0889 - accuracy: 0.9910 - val_loss: 0.3331 - val_accuracy: 0.9032\n","Epoch 31/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.0851 - accuracy: 0.9920 - val_loss: 0.3402 - val_accuracy: 0.8978\n","Epoch 32/32\n","32/32 [==============================] - 0s 16ms/step - loss: 0.0795 - accuracy: 0.9960 - val_loss: 0.3363 - val_accuracy: 0.9010\n","Epoch 1/32\n","32/32 [==============================] - 1s 30ms/step - loss: 1.9323 - accuracy: 0.4240 - val_loss: 1.5211 - val_accuracy: 0.6530\n","Epoch 2/32\n","32/32 [==============================] - 0s 15ms/step - loss: 1.2354 - accuracy: 0.7360 - val_loss: 1.0214 - val_accuracy: 0.7532\n","Epoch 3/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.8516 - accuracy: 0.8260 - val_loss: 0.7837 - val_accuracy: 0.8068\n","Epoch 4/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.6497 - accuracy: 0.8540 - val_loss: 0.6324 - val_accuracy: 0.8422\n","Epoch 5/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.5399 - accuracy: 0.8770 - val_loss: 0.5687 - val_accuracy: 0.8464\n","Epoch 6/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.4638 - accuracy: 0.8910 - val_loss: 0.5388 - val_accuracy: 0.8484\n","Epoch 7/32\n","32/32 [==============================] - 1s 27ms/step - loss: 0.4172 - accuracy: 0.9030 - val_loss: 0.4867 - val_accuracy: 0.8598\n","Epoch 8/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.3742 - accuracy: 0.9000 - val_loss: 0.4825 - val_accuracy: 0.8572\n","Epoch 9/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.3379 - accuracy: 0.9160 - val_loss: 0.4639 - val_accuracy: 0.8596\n","Epoch 10/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.3107 - accuracy: 0.9330 - val_loss: 0.4172 - val_accuracy: 0.8758\n","Epoch 11/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.2876 - accuracy: 0.9380 - val_loss: 0.4158 - val_accuracy: 0.8770\n","Epoch 12/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.2596 - accuracy: 0.9440 - val_loss: 0.3917 - val_accuracy: 0.8868\n","Epoch 13/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.2453 - accuracy: 0.9500 - val_loss: 0.3830 - val_accuracy: 0.8864\n","Epoch 14/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.2259 - accuracy: 0.9550 - val_loss: 0.4146 - val_accuracy: 0.8772\n","Epoch 15/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.2134 - accuracy: 0.9590 - val_loss: 0.3652 - val_accuracy: 0.8906\n","Epoch 16/32\n","32/32 [==============================] - 1s 30ms/step - loss: 0.1953 - accuracy: 0.9620 - val_loss: 0.3664 - val_accuracy: 0.8898\n","Epoch 17/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.1829 - accuracy: 0.9680 - val_loss: 0.3828 - val_accuracy: 0.8822\n","Epoch 18/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.1738 - accuracy: 0.9690 - val_loss: 0.3597 - val_accuracy: 0.8868\n","Epoch 19/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.1611 - accuracy: 0.9750 - val_loss: 0.3670 - val_accuracy: 0.8866\n","Epoch 20/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1510 - accuracy: 0.9750 - val_loss: 0.3572 - val_accuracy: 0.8888\n","Epoch 21/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.1416 - accuracy: 0.9840 - val_loss: 0.4440 - val_accuracy: 0.8510\n","Epoch 22/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.1383 - accuracy: 0.9790 - val_loss: 0.3489 - val_accuracy: 0.8928\n","Epoch 23/32\n","32/32 [==============================] - 1s 22ms/step - loss: 0.1267 - accuracy: 0.9870 - val_loss: 0.3571 - val_accuracy: 0.8886\n","Epoch 24/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.1194 - accuracy: 0.9860 - val_loss: 0.3510 - val_accuracy: 0.8926\n","Epoch 25/32\n","32/32 [==============================] - 1s 17ms/step - loss: 0.1135 - accuracy: 0.9880 - val_loss: 0.3642 - val_accuracy: 0.8850\n","Epoch 26/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1078 - accuracy: 0.9900 - val_loss: 0.3559 - val_accuracy: 0.8878\n","Epoch 27/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1007 - accuracy: 0.9920 - val_loss: 0.3512 - val_accuracy: 0.8904\n","Epoch 28/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.0969 - accuracy: 0.9920 - val_loss: 0.3503 - val_accuracy: 0.8910\n","Epoch 29/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.0910 - accuracy: 0.9960 - val_loss: 0.3436 - val_accuracy: 0.8956\n","Epoch 30/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.0858 - accuracy: 0.9950 - val_loss: 0.3528 - val_accuracy: 0.8896\n","Epoch 31/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0821 - accuracy: 0.9960 - val_loss: 0.3427 - val_accuracy: 0.8950\n","Epoch 32/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.0773 - accuracy: 0.9960 - val_loss: 0.3424 - val_accuracy: 0.8966\n","Epoch 1/32\n","32/32 [==============================] - 2s 34ms/step - loss: 2.0401 - accuracy: 0.3870 - val_loss: 1.6847 - val_accuracy: 0.5928\n","Epoch 2/32\n","32/32 [==============================] - 1s 29ms/step - loss: 1.3588 - accuracy: 0.7130 - val_loss: 1.1246 - val_accuracy: 0.7546\n","Epoch 3/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.8870 - accuracy: 0.8210 - val_loss: 0.8614 - val_accuracy: 0.7852\n","Epoch 4/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.6509 - accuracy: 0.8620 - val_loss: 0.6888 - val_accuracy: 0.8142\n","Epoch 5/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.5243 - accuracy: 0.8780 - val_loss: 0.5737 - val_accuracy: 0.8528\n","Epoch 6/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.4399 - accuracy: 0.8950 - val_loss: 0.5360 - val_accuracy: 0.8550\n","Epoch 7/32\n","32/32 [==============================] - 1s 21ms/step - loss: 0.3899 - accuracy: 0.9110 - val_loss: 0.4637 - val_accuracy: 0.8748\n","Epoch 8/32\n","32/32 [==============================] - 0s 16ms/step - loss: 0.3402 - accuracy: 0.9200 - val_loss: 0.4441 - val_accuracy: 0.8752\n","Epoch 9/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.3052 - accuracy: 0.9320 - val_loss: 0.4410 - val_accuracy: 0.8688\n","Epoch 10/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.2845 - accuracy: 0.9380 - val_loss: 0.4294 - val_accuracy: 0.8736\n","Epoch 11/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.2582 - accuracy: 0.9470 - val_loss: 0.4326 - val_accuracy: 0.8686\n","Epoch 12/32\n","32/32 [==============================] - 1s 26ms/step - loss: 0.2366 - accuracy: 0.9540 - val_loss: 0.4026 - val_accuracy: 0.8806\n","Epoch 13/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.2170 - accuracy: 0.9520 - val_loss: 0.3933 - val_accuracy: 0.8832\n","Epoch 14/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.2036 - accuracy: 0.9540 - val_loss: 0.3909 - val_accuracy: 0.8818\n","Epoch 15/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1916 - accuracy: 0.9520 - val_loss: 0.4866 - val_accuracy: 0.8412\n","Epoch 16/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1793 - accuracy: 0.9680 - val_loss: 0.3712 - val_accuracy: 0.8870\n","Epoch 17/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1639 - accuracy: 0.9650 - val_loss: 0.4574 - val_accuracy: 0.8526\n","Epoch 18/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1561 - accuracy: 0.9680 - val_loss: 0.3833 - val_accuracy: 0.8856\n","Epoch 19/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1456 - accuracy: 0.9740 - val_loss: 0.3719 - val_accuracy: 0.8874\n","Epoch 20/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.1356 - accuracy: 0.9760 - val_loss: 0.3890 - val_accuracy: 0.8822\n","Epoch 21/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1269 - accuracy: 0.9840 - val_loss: 0.3704 - val_accuracy: 0.8874\n","Epoch 22/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1199 - accuracy: 0.9830 - val_loss: 0.3636 - val_accuracy: 0.8908\n","Epoch 23/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1131 - accuracy: 0.9840 - val_loss: 0.3553 - val_accuracy: 0.8930\n","Epoch 24/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1057 - accuracy: 0.9870 - val_loss: 0.3673 - val_accuracy: 0.8870\n","Epoch 25/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1009 - accuracy: 0.9880 - val_loss: 0.3611 - val_accuracy: 0.8884\n","Epoch 26/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0971 - accuracy: 0.9870 - val_loss: 0.3500 - val_accuracy: 0.8960\n","Epoch 27/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0908 - accuracy: 0.9880 - val_loss: 0.3707 - val_accuracy: 0.8866\n","Epoch 28/32\n","32/32 [==============================] - 1s 22ms/step - loss: 0.0867 - accuracy: 0.9890 - val_loss: 0.3467 - val_accuracy: 0.8940\n","Epoch 29/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.0809 - accuracy: 0.9920 - val_loss: 0.3480 - val_accuracy: 0.8964\n","Epoch 30/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.0767 - accuracy: 0.9920 - val_loss: 0.3542 - val_accuracy: 0.8918\n","Epoch 31/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0744 - accuracy: 0.9910 - val_loss: 0.3535 - val_accuracy: 0.8934\n","Epoch 32/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.0708 - accuracy: 0.9940 - val_loss: 0.3578 - val_accuracy: 0.8916\n","Epoch 1/32\n","32/32 [==============================] - 1s 29ms/step - loss: 1.9846 - accuracy: 0.4010 - val_loss: 1.6562 - val_accuracy: 0.5974\n","Epoch 2/32\n","32/32 [==============================] - 1s 28ms/step - loss: 1.3260 - accuracy: 0.7030 - val_loss: 1.0929 - val_accuracy: 0.7526\n","Epoch 3/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.8940 - accuracy: 0.8030 - val_loss: 0.8444 - val_accuracy: 0.7928\n","Epoch 4/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.6816 - accuracy: 0.8490 - val_loss: 0.6816 - val_accuracy: 0.8274\n","Epoch 5/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.5569 - accuracy: 0.8680 - val_loss: 0.5697 - val_accuracy: 0.8524\n","Epoch 6/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.4715 - accuracy: 0.8970 - val_loss: 0.5039 - val_accuracy: 0.8712\n","Epoch 7/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.4097 - accuracy: 0.9010 - val_loss: 0.5004 - val_accuracy: 0.8522\n","Epoch 8/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.3714 - accuracy: 0.9160 - val_loss: 0.4507 - val_accuracy: 0.8718\n","Epoch 9/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.3345 - accuracy: 0.9240 - val_loss: 0.4215 - val_accuracy: 0.8818\n","Epoch 10/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.2988 - accuracy: 0.9350 - val_loss: 0.4133 - val_accuracy: 0.8802\n","Epoch 11/32\n","32/32 [==============================] - 1s 18ms/step - loss: 0.2741 - accuracy: 0.9440 - val_loss: 0.4365 - val_accuracy: 0.8692\n","Epoch 12/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.2565 - accuracy: 0.9460 - val_loss: 0.3973 - val_accuracy: 0.8804\n","Epoch 13/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.2348 - accuracy: 0.9530 - val_loss: 0.3863 - val_accuracy: 0.8862\n","Epoch 14/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.2186 - accuracy: 0.9560 - val_loss: 0.3833 - val_accuracy: 0.8824\n","Epoch 15/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.2005 - accuracy: 0.9680 - val_loss: 0.3668 - val_accuracy: 0.8904\n","Epoch 16/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1871 - accuracy: 0.9730 - val_loss: 0.3576 - val_accuracy: 0.8934\n","Epoch 17/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1740 - accuracy: 0.9670 - val_loss: 0.3538 - val_accuracy: 0.8914\n","Epoch 18/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1638 - accuracy: 0.9720 - val_loss: 0.3793 - val_accuracy: 0.8830\n","Epoch 19/32\n","32/32 [==============================] - 0s 16ms/step - loss: 0.1563 - accuracy: 0.9720 - val_loss: 0.3790 - val_accuracy: 0.8818\n","Epoch 20/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1447 - accuracy: 0.9780 - val_loss: 0.3477 - val_accuracy: 0.8930\n","Epoch 21/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1361 - accuracy: 0.9810 - val_loss: 0.3552 - val_accuracy: 0.8900\n","Epoch 22/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1284 - accuracy: 0.9850 - val_loss: 0.3497 - val_accuracy: 0.8942\n","Epoch 23/32\n","32/32 [==============================] - 1s 19ms/step - loss: 0.1218 - accuracy: 0.9830 - val_loss: 0.3537 - val_accuracy: 0.8910\n","Epoch 24/32\n","32/32 [==============================] - 1s 27ms/step - loss: 0.1144 - accuracy: 0.9900 - val_loss: 0.3475 - val_accuracy: 0.8904\n","Epoch 25/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.1096 - accuracy: 0.9910 - val_loss: 0.3383 - val_accuracy: 0.8982\n","Epoch 26/32\n","32/32 [==============================] - 1s 22ms/step - loss: 0.1014 - accuracy: 0.9890 - val_loss: 0.3357 - val_accuracy: 0.8980\n","Epoch 27/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0959 - accuracy: 0.9920 - val_loss: 0.3489 - val_accuracy: 0.8932\n","Epoch 28/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0917 - accuracy: 0.9930 - val_loss: 0.3385 - val_accuracy: 0.8966\n","Epoch 29/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0870 - accuracy: 0.9930 - val_loss: 0.3411 - val_accuracy: 0.8958\n","Epoch 30/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0827 - accuracy: 0.9940 - val_loss: 0.3425 - val_accuracy: 0.8924\n","Epoch 31/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0784 - accuracy: 0.9950 - val_loss: 0.3392 - val_accuracy: 0.8962\n","Epoch 32/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.0751 - accuracy: 0.9950 - val_loss: 0.3366 - val_accuracy: 0.8992\n","Epoch 1/32\n","32/32 [==============================] - 1s 19ms/step - loss: 2.0149 - accuracy: 0.4060 - val_loss: 1.6529 - val_accuracy: 0.6614\n","Epoch 2/32\n","32/32 [==============================] - 0s 13ms/step - loss: 1.3303 - accuracy: 0.7590 - val_loss: 1.0901 - val_accuracy: 0.7754\n","Epoch 3/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.8756 - accuracy: 0.8270 - val_loss: 0.8616 - val_accuracy: 0.7604\n","Epoch 4/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.6545 - accuracy: 0.8440 - val_loss: 0.6998 - val_accuracy: 0.8116\n","Epoch 5/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.5367 - accuracy: 0.8800 - val_loss: 0.6264 - val_accuracy: 0.8220\n","Epoch 6/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.4528 - accuracy: 0.8990 - val_loss: 0.5552 - val_accuracy: 0.8436\n","Epoch 7/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.4008 - accuracy: 0.9130 - val_loss: 0.4946 - val_accuracy: 0.8606\n","Epoch 8/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.3536 - accuracy: 0.9170 - val_loss: 0.4728 - val_accuracy: 0.8622\n","Epoch 9/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.3191 - accuracy: 0.9310 - val_loss: 0.4493 - val_accuracy: 0.8702\n","Epoch 10/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.2941 - accuracy: 0.9320 - val_loss: 0.4424 - val_accuracy: 0.8700\n","Epoch 11/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.2692 - accuracy: 0.9460 - val_loss: 0.4169 - val_accuracy: 0.8772\n","Epoch 12/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.2509 - accuracy: 0.9440 - val_loss: 0.4061 - val_accuracy: 0.8816\n","Epoch 13/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.2295 - accuracy: 0.9540 - val_loss: 0.4053 - val_accuracy: 0.8822\n","Epoch 14/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.2116 - accuracy: 0.9610 - val_loss: 0.3953 - val_accuracy: 0.8826\n","Epoch 15/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1987 - accuracy: 0.9630 - val_loss: 0.4027 - val_accuracy: 0.8780\n","Epoch 16/32\n","32/32 [==============================] - 0s 16ms/step - loss: 0.1883 - accuracy: 0.9650 - val_loss: 0.3878 - val_accuracy: 0.8874\n","Epoch 17/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.1741 - accuracy: 0.9670 - val_loss: 0.4308 - val_accuracy: 0.8692\n","Epoch 18/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.1663 - accuracy: 0.9700 - val_loss: 0.3801 - val_accuracy: 0.8892\n","Epoch 19/32\n","32/32 [==============================] - 1s 27ms/step - loss: 0.1529 - accuracy: 0.9760 - val_loss: 0.4239 - val_accuracy: 0.8682\n","Epoch 20/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.1486 - accuracy: 0.9780 - val_loss: 0.3758 - val_accuracy: 0.8832\n","Epoch 21/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.1361 - accuracy: 0.9780 - val_loss: 0.3674 - val_accuracy: 0.8880\n","Epoch 22/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.1267 - accuracy: 0.9790 - val_loss: 0.3653 - val_accuracy: 0.8898\n","Epoch 23/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.1185 - accuracy: 0.9810 - val_loss: 0.3637 - val_accuracy: 0.8908\n","Epoch 24/32\n","32/32 [==============================] - 1s 22ms/step - loss: 0.1124 - accuracy: 0.9860 - val_loss: 0.3611 - val_accuracy: 0.8914\n","Epoch 25/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.1069 - accuracy: 0.9870 - val_loss: 0.3617 - val_accuracy: 0.8914\n","Epoch 26/32\n","32/32 [==============================] - 1s 16ms/step - loss: 0.0997 - accuracy: 0.9880 - val_loss: 0.3611 - val_accuracy: 0.8908\n","Epoch 27/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.0951 - accuracy: 0.9870 - val_loss: 0.3661 - val_accuracy: 0.8900\n","Epoch 28/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0887 - accuracy: 0.9920 - val_loss: 0.3628 - val_accuracy: 0.8910\n","Epoch 29/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0859 - accuracy: 0.9930 - val_loss: 0.3679 - val_accuracy: 0.8886\n","Epoch 30/32\n","32/32 [==============================] - 0s 13ms/step - loss: 0.0831 - accuracy: 0.9920 - val_loss: 0.3650 - val_accuracy: 0.8920\n","Epoch 31/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0772 - accuracy: 0.9950 - val_loss: 0.3587 - val_accuracy: 0.8922\n","Epoch 32/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0738 - accuracy: 0.9960 - val_loss: 0.3575 - val_accuracy: 0.8934\n","Epoch 1/32\n","32/32 [==============================] - 2s 20ms/step - loss: 2.0304 - accuracy: 0.3610 - val_loss: 1.7056 - val_accuracy: 0.6408\n","Epoch 2/32\n","32/32 [==============================] - 0s 14ms/step - loss: 1.3555 - accuracy: 0.7580 - val_loss: 1.1317 - val_accuracy: 0.7624\n","Epoch 3/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.8886 - accuracy: 0.8390 - val_loss: 0.9214 - val_accuracy: 0.7520\n","Epoch 4/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.6581 - accuracy: 0.8640 - val_loss: 0.7138 - val_accuracy: 0.8206\n","Epoch 5/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.5339 - accuracy: 0.8850 - val_loss: 0.6006 - val_accuracy: 0.8336\n","Epoch 6/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.4573 - accuracy: 0.8950 - val_loss: 0.5418 - val_accuracy: 0.8498\n","Epoch 7/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.4013 - accuracy: 0.9130 - val_loss: 0.5020 - val_accuracy: 0.8656\n","Epoch 8/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.3571 - accuracy: 0.9260 - val_loss: 0.4842 - val_accuracy: 0.8594\n","Epoch 9/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.3259 - accuracy: 0.9270 - val_loss: 0.4962 - val_accuracy: 0.8456\n","Epoch 10/32\n","32/32 [==============================] - 1s 26ms/step - loss: 0.2997 - accuracy: 0.9260 - val_loss: 0.4385 - val_accuracy: 0.8734\n","Epoch 11/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.2688 - accuracy: 0.9430 - val_loss: 0.4254 - val_accuracy: 0.8760\n","Epoch 12/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.2533 - accuracy: 0.9460 - val_loss: 0.4017 - val_accuracy: 0.8864\n","Epoch 13/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.2352 - accuracy: 0.9460 - val_loss: 0.3966 - val_accuracy: 0.8844\n","Epoch 14/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.2209 - accuracy: 0.9580 - val_loss: 0.3879 - val_accuracy: 0.8848\n","Epoch 15/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.2053 - accuracy: 0.9630 - val_loss: 0.3798 - val_accuracy: 0.8884\n","Epoch 16/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.1914 - accuracy: 0.9690 - val_loss: 0.3792 - val_accuracy: 0.8870\n","Epoch 17/32\n","32/32 [==============================] - 1s 23ms/step - loss: 0.1804 - accuracy: 0.9650 - val_loss: 0.3763 - val_accuracy: 0.8888\n","Epoch 18/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.1669 - accuracy: 0.9750 - val_loss: 0.3706 - val_accuracy: 0.8898\n","Epoch 19/32\n","32/32 [==============================] - 1s 17ms/step - loss: 0.1587 - accuracy: 0.9720 - val_loss: 0.4104 - val_accuracy: 0.8822\n","Epoch 20/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1525 - accuracy: 0.9720 - val_loss: 0.3734 - val_accuracy: 0.8836\n","Epoch 21/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1427 - accuracy: 0.9750 - val_loss: 0.3689 - val_accuracy: 0.8910\n","Epoch 22/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1324 - accuracy: 0.9790 - val_loss: 0.3580 - val_accuracy: 0.8912\n","Epoch 23/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1258 - accuracy: 0.9800 - val_loss: 0.3605 - val_accuracy: 0.8876\n","Epoch 24/32\n","32/32 [==============================] - 0s 16ms/step - loss: 0.1183 - accuracy: 0.9790 - val_loss: 0.3554 - val_accuracy: 0.8942\n","Epoch 25/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1122 - accuracy: 0.9860 - val_loss: 0.3765 - val_accuracy: 0.8820\n","Epoch 26/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1074 - accuracy: 0.9820 - val_loss: 0.4004 - val_accuracy: 0.8790\n","Epoch 27/32\n","32/32 [==============================] - 1s 26ms/step - loss: 0.1040 - accuracy: 0.9870 - val_loss: 0.3547 - val_accuracy: 0.8928\n","Epoch 28/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0955 - accuracy: 0.9920 - val_loss: 0.3532 - val_accuracy: 0.8936\n","Epoch 29/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.0906 - accuracy: 0.9910 - val_loss: 0.3541 - val_accuracy: 0.8924\n","Epoch 30/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0869 - accuracy: 0.9950 - val_loss: 0.3640 - val_accuracy: 0.8876\n","Epoch 31/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.0828 - accuracy: 0.9930 - val_loss: 0.3482 - val_accuracy: 0.8958\n","Epoch 32/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.0794 - accuracy: 0.9950 - val_loss: 0.3454 - val_accuracy: 0.8978\n","Epoch 1/32\n","32/32 [==============================] - 1s 20ms/step - loss: 1.9892 - accuracy: 0.3730 - val_loss: 1.6508 - val_accuracy: 0.6006\n","Epoch 2/32\n","32/32 [==============================] - 1s 25ms/step - loss: 1.3266 - accuracy: 0.6980 - val_loss: 1.1349 - val_accuracy: 0.7224\n","Epoch 3/32\n","32/32 [==============================] - 1s 20ms/step - loss: 0.8956 - accuracy: 0.7880 - val_loss: 0.8376 - val_accuracy: 0.7914\n","Epoch 4/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.6746 - accuracy: 0.8320 - val_loss: 0.6855 - val_accuracy: 0.8180\n","Epoch 5/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.5484 - accuracy: 0.8640 - val_loss: 0.6135 - val_accuracy: 0.8366\n","Epoch 6/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.4665 - accuracy: 0.8840 - val_loss: 0.5448 - val_accuracy: 0.8434\n","Epoch 7/32\n","32/32 [==============================] - 1s 27ms/step - loss: 0.4055 - accuracy: 0.8930 - val_loss: 0.5092 - val_accuracy: 0.8590\n","Epoch 8/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.3618 - accuracy: 0.9050 - val_loss: 0.4875 - val_accuracy: 0.8600\n","Epoch 9/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.3307 - accuracy: 0.9200 - val_loss: 0.4839 - val_accuracy: 0.8620\n","Epoch 10/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.3000 - accuracy: 0.9280 - val_loss: 0.4353 - val_accuracy: 0.8800\n","Epoch 11/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.2753 - accuracy: 0.9290 - val_loss: 0.4198 - val_accuracy: 0.8776\n","Epoch 12/32\n","32/32 [==============================] - 1s 21ms/step - loss: 0.2496 - accuracy: 0.9350 - val_loss: 0.4330 - val_accuracy: 0.8654\n","Epoch 13/32\n","32/32 [==============================] - 1s 17ms/step - loss: 0.2342 - accuracy: 0.9470 - val_loss: 0.4019 - val_accuracy: 0.8838\n","Epoch 14/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.2149 - accuracy: 0.9480 - val_loss: 0.3958 - val_accuracy: 0.8824\n","Epoch 15/32\n","32/32 [==============================] - 1s 26ms/step - loss: 0.2009 - accuracy: 0.9550 - val_loss: 0.3926 - val_accuracy: 0.8850\n","Epoch 16/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1843 - accuracy: 0.9630 - val_loss: 0.3829 - val_accuracy: 0.8850\n","Epoch 17/32\n","32/32 [==============================] - 1s 16ms/step - loss: 0.1738 - accuracy: 0.9690 - val_loss: 0.3796 - val_accuracy: 0.8852\n","Epoch 18/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1643 - accuracy: 0.9700 - val_loss: 0.3794 - val_accuracy: 0.8802\n","Epoch 19/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1526 - accuracy: 0.9750 - val_loss: 0.3707 - val_accuracy: 0.8818\n","Epoch 20/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1446 - accuracy: 0.9730 - val_loss: 0.3677 - val_accuracy: 0.8916\n","Epoch 21/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.1344 - accuracy: 0.9810 - val_loss: 0.3653 - val_accuracy: 0.8888\n","Epoch 22/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1274 - accuracy: 0.9820 - val_loss: 0.3782 - val_accuracy: 0.8854\n","Epoch 23/32\n","32/32 [==============================] - 1s 16ms/step - loss: 0.1207 - accuracy: 0.9820 - val_loss: 0.3753 - val_accuracy: 0.8854\n","Epoch 24/32\n","32/32 [==============================] - 1s 16ms/step - loss: 0.1140 - accuracy: 0.9840 - val_loss: 0.3638 - val_accuracy: 0.8908\n","Epoch 25/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1066 - accuracy: 0.9830 - val_loss: 0.3621 - val_accuracy: 0.8880\n","Epoch 26/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1010 - accuracy: 0.9870 - val_loss: 0.3566 - val_accuracy: 0.8926\n","Epoch 27/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0970 - accuracy: 0.9900 - val_loss: 0.3550 - val_accuracy: 0.8964\n","Epoch 28/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0916 - accuracy: 0.9900 - val_loss: 0.3560 - val_accuracy: 0.8930\n","Epoch 29/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0855 - accuracy: 0.9900 - val_loss: 0.3532 - val_accuracy: 0.8926\n","Epoch 30/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.0821 - accuracy: 0.9920 - val_loss: 0.3570 - val_accuracy: 0.8900\n","Epoch 31/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0784 - accuracy: 0.9930 - val_loss: 0.3588 - val_accuracy: 0.8872\n","Epoch 32/32\n","32/32 [==============================] - 1s 24ms/step - loss: 0.0773 - accuracy: 0.9930 - val_loss: 0.3540 - val_accuracy: 0.8914\n","Epoch 1/32\n","32/32 [==============================] - 2s 35ms/step - loss: 2.0067 - accuracy: 0.3950 - val_loss: 1.6491 - val_accuracy: 0.6286\n","Epoch 2/32\n","32/32 [==============================] - 1s 29ms/step - loss: 1.3345 - accuracy: 0.7440 - val_loss: 1.0830 - val_accuracy: 0.7956\n","Epoch 3/32\n","32/32 [==============================] - 1s 30ms/step - loss: 0.8827 - accuracy: 0.8340 - val_loss: 0.7845 - val_accuracy: 0.8296\n","Epoch 4/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.6592 - accuracy: 0.8640 - val_loss: 0.6415 - val_accuracy: 0.8418\n","Epoch 5/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.5365 - accuracy: 0.8850 - val_loss: 0.5996 - val_accuracy: 0.8494\n","Epoch 6/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.4566 - accuracy: 0.8890 - val_loss: 0.5286 - val_accuracy: 0.8588\n","Epoch 7/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.3998 - accuracy: 0.9100 - val_loss: 0.4688 - val_accuracy: 0.8762\n","Epoch 8/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.3559 - accuracy: 0.9120 - val_loss: 0.4838 - val_accuracy: 0.8582\n","Epoch 9/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.3216 - accuracy: 0.9220 - val_loss: 0.4294 - val_accuracy: 0.8810\n","Epoch 10/32\n","32/32 [==============================] - 0s 14ms/step - loss: 0.2922 - accuracy: 0.9340 - val_loss: 0.4225 - val_accuracy: 0.8788\n","Epoch 11/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.2655 - accuracy: 0.9400 - val_loss: 0.4259 - val_accuracy: 0.8768\n","Epoch 12/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.2477 - accuracy: 0.9360 - val_loss: 0.4114 - val_accuracy: 0.8836\n","Epoch 13/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.2282 - accuracy: 0.9490 - val_loss: 0.3851 - val_accuracy: 0.8854\n","Epoch 14/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.2124 - accuracy: 0.9520 - val_loss: 0.3863 - val_accuracy: 0.8870\n","Epoch 15/32\n","32/32 [==============================] - 1s 16ms/step - loss: 0.1924 - accuracy: 0.9580 - val_loss: 0.4005 - val_accuracy: 0.8814\n","Epoch 16/32\n","32/32 [==============================] - 0s 16ms/step - loss: 0.1824 - accuracy: 0.9690 - val_loss: 0.3851 - val_accuracy: 0.8868\n","Epoch 17/32\n","32/32 [==============================] - 1s 26ms/step - loss: 0.1683 - accuracy: 0.9720 - val_loss: 0.3856 - val_accuracy: 0.8846\n","Epoch 18/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1582 - accuracy: 0.9750 - val_loss: 0.3740 - val_accuracy: 0.8932\n","Epoch 19/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1500 - accuracy: 0.9780 - val_loss: 0.3839 - val_accuracy: 0.8856\n","Epoch 20/32\n","32/32 [==============================] - 1s 26ms/step - loss: 0.1397 - accuracy: 0.9770 - val_loss: 0.3539 - val_accuracy: 0.8960\n","Epoch 21/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1309 - accuracy: 0.9790 - val_loss: 0.3722 - val_accuracy: 0.8894\n","Epoch 22/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1229 - accuracy: 0.9850 - val_loss: 0.3893 - val_accuracy: 0.8852\n","Epoch 23/32\n","32/32 [==============================] - 0s 15ms/step - loss: 0.1186 - accuracy: 0.9840 - val_loss: 0.3504 - val_accuracy: 0.8950\n","Epoch 24/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.1083 - accuracy: 0.9880 - val_loss: 0.3869 - val_accuracy: 0.8848\n","Epoch 25/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.1057 - accuracy: 0.9870 - val_loss: 0.3518 - val_accuracy: 0.8994\n","Epoch 26/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0973 - accuracy: 0.9890 - val_loss: 0.3487 - val_accuracy: 0.8974\n","Epoch 27/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0913 - accuracy: 0.9870 - val_loss: 0.3679 - val_accuracy: 0.8844\n","Epoch 28/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.0873 - accuracy: 0.9900 - val_loss: 0.3450 - val_accuracy: 0.8974\n","Epoch 29/32\n","32/32 [==============================] - 1s 28ms/step - loss: 0.0807 - accuracy: 0.9920 - val_loss: 0.3442 - val_accuracy: 0.8980\n","Epoch 30/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.0770 - accuracy: 0.9920 - val_loss: 0.3448 - val_accuracy: 0.8982\n","Epoch 31/32\n","32/32 [==============================] - 1s 29ms/step - loss: 0.0737 - accuracy: 0.9930 - val_loss: 0.3442 - val_accuracy: 0.8982\n","Epoch 32/32\n","32/32 [==============================] - 1s 25ms/step - loss: 0.0692 - accuracy: 0.9920 - val_loss: 0.3531 - val_accuracy: 0.8970\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n","  warnings.warn('`model.predict_proba()` is deprecated and '\n"]}]},{"cell_type":"code","source":["# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n","amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n","\n","# Fit the attack models.\n","print(\"Training the attack models...\")\n","amb.fit(X_shadow, y_shadow, fit_kwargs=dict(epochs=32, verbose=True)\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Duf01H4_CZ_Y","executionInfo":{"status":"ok","timestamp":1676597869410,"user_tz":300,"elapsed":95815,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"0ab3f086-38d1-4513-8f46-1aeda6cd9f5b"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["Training the attack models...\n","Epoch 1/32\n","62/62 [==============================] - 1s 5ms/step - loss: 0.6909 - accuracy: 0.5288\n","Epoch 2/32\n","62/62 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.5482\n","Epoch 3/32\n","62/62 [==============================] - 0s 4ms/step - loss: 0.6796 - accuracy: 0.5273\n","Epoch 4/32\n","62/62 [==============================] - 0s 5ms/step - loss: 0.6716 - accuracy: 0.5365\n","Epoch 5/32\n","62/62 [==============================] - 0s 4ms/step - loss: 0.6679 - accuracy: 0.5656\n","Epoch 6/32\n","62/62 [==============================] - 0s 4ms/step - loss: 0.6681 - accuracy: 0.5462\n","Epoch 7/32\n","62/62 [==============================] - 0s 4ms/step - loss: 0.6658 - accuracy: 0.5482\n","Epoch 8/32\n","62/62 [==============================] - 0s 4ms/step - loss: 0.6631 - accuracy: 0.5539\n","Epoch 9/32\n","62/62 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5564\n","Epoch 10/32\n","62/62 [==============================] - 0s 4ms/step - loss: 0.6639 - accuracy: 0.5554\n","Epoch 11/32\n","62/62 [==============================] - 0s 5ms/step - loss: 0.6590 - accuracy: 0.5630\n","Epoch 12/32\n","62/62 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5493\n","Epoch 13/32\n","62/62 [==============================] - 0s 3ms/step - loss: 0.6622 - accuracy: 0.5493\n","Epoch 14/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6604 - accuracy: 0.5641\n","Epoch 15/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6609 - accuracy: 0.5620\n","Epoch 16/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6605 - accuracy: 0.5605\n","Epoch 17/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6603 - accuracy: 0.5636\n","Epoch 18/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6594 - accuracy: 0.5615\n","Epoch 19/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6562 - accuracy: 0.5671\n","Epoch 20/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6591 - accuracy: 0.5595\n","Epoch 21/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6588 - accuracy: 0.5605\n","Epoch 22/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.5615\n","Epoch 23/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6596 - accuracy: 0.5533\n","Epoch 24/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6580 - accuracy: 0.5641\n","Epoch 25/32\n","62/62 [==============================] - 0s 3ms/step - loss: 0.6569 - accuracy: 0.5646\n","Epoch 26/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.5595\n","Epoch 27/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6572 - accuracy: 0.5610\n","Epoch 28/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6588 - accuracy: 0.5574\n","Epoch 29/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6558 - accuracy: 0.5605\n","Epoch 30/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6569 - accuracy: 0.5600\n","Epoch 31/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6551 - accuracy: 0.5610\n","Epoch 32/32\n","62/62 [==============================] - 0s 2ms/step - loss: 0.6552 - accuracy: 0.5651\n","Epoch 1/32\n","71/71 [==============================] - 1s 2ms/step - loss: 0.6947 - accuracy: 0.5118\n","Epoch 2/32\n","71/71 [==============================] - 0s 3ms/step - loss: 0.6933 - accuracy: 0.5105\n","Epoch 3/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5132\n","Epoch 4/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5172\n","Epoch 5/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.5256\n","Epoch 6/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6875 - accuracy: 0.4967\n","Epoch 7/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6844 - accuracy: 0.5074\n","Epoch 8/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.5029\n","Epoch 9/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.5096\n","Epoch 10/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6819 - accuracy: 0.5230\n","Epoch 11/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6817 - accuracy: 0.5288\n","Epoch 12/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6833 - accuracy: 0.5207\n","Epoch 13/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6815 - accuracy: 0.5372\n","Epoch 14/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5118\n","Epoch 15/32\n","71/71 [==============================] - 0s 3ms/step - loss: 0.6820 - accuracy: 0.5047\n","Epoch 16/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.5109\n","Epoch 17/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.5207\n","Epoch 18/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6821 - accuracy: 0.5069\n","Epoch 19/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6806 - accuracy: 0.5091\n","Epoch 20/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6820 - accuracy: 0.5033\n","Epoch 21/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6828 - accuracy: 0.4980\n","Epoch 22/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5051\n","Epoch 23/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.5132\n","Epoch 24/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6823 - accuracy: 0.5176\n","Epoch 25/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.5158\n","Epoch 26/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6813 - accuracy: 0.5100\n","Epoch 27/32\n","71/71 [==============================] - 0s 3ms/step - loss: 0.6805 - accuracy: 0.5109\n","Epoch 28/32\n","71/71 [==============================] - 0s 3ms/step - loss: 0.6809 - accuracy: 0.5225\n","Epoch 29/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6808 - accuracy: 0.5007\n","Epoch 30/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6806 - accuracy: 0.5167\n","Epoch 31/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6812 - accuracy: 0.5096\n","Epoch 32/32\n","71/71 [==============================] - 0s 2ms/step - loss: 0.6801 - accuracy: 0.5127\n","Epoch 1/32\n","66/66 [==============================] - 1s 4ms/step - loss: 0.6925 - accuracy: 0.5216\n","Epoch 2/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6785 - accuracy: 0.5609\n","Epoch 3/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6593 - accuracy: 0.5873\n","Epoch 4/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6527 - accuracy: 0.5840\n","Epoch 5/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6510 - accuracy: 0.5893\n","Epoch 6/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.5873\n","Epoch 7/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6469 - accuracy: 0.5936\n","Epoch 8/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6481 - accuracy: 0.5888\n","Epoch 9/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6457 - accuracy: 0.5893\n","Epoch 10/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6471 - accuracy: 0.5893\n","Epoch 11/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.5907\n","Epoch 12/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6432 - accuracy: 0.5945\n","Epoch 13/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6464 - accuracy: 0.5888\n","Epoch 14/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6437 - accuracy: 0.5926\n","Epoch 15/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6420 - accuracy: 0.5921\n","Epoch 16/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6452 - accuracy: 0.5917\n","Epoch 17/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6413 - accuracy: 0.5883\n","Epoch 18/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6411 - accuracy: 0.5917\n","Epoch 19/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6427 - accuracy: 0.5907\n","Epoch 20/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.5917\n","Epoch 21/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6403 - accuracy: 0.5912\n","Epoch 22/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6406 - accuracy: 0.5907\n","Epoch 23/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6415 - accuracy: 0.5931\n","Epoch 24/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6409 - accuracy: 0.5936\n","Epoch 25/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6402 - accuracy: 0.5917\n","Epoch 26/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6411 - accuracy: 0.5931\n","Epoch 27/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6393 - accuracy: 0.5940\n","Epoch 28/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6404 - accuracy: 0.5950\n","Epoch 29/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6403 - accuracy: 0.5940\n","Epoch 30/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6391 - accuracy: 0.5955\n","Epoch 31/32\n","66/66 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.5926\n","Epoch 32/32\n","66/66 [==============================] - 0s 3ms/step - loss: 0.6399 - accuracy: 0.5936\n","Epoch 1/32\n","64/64 [==============================] - 1s 4ms/step - loss: 0.6851 - accuracy: 0.5633\n","Epoch 2/32\n","64/64 [==============================] - 0s 5ms/step - loss: 0.6729 - accuracy: 0.5579\n","Epoch 3/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6567 - accuracy: 0.5735\n","Epoch 4/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6493 - accuracy: 0.5848\n","Epoch 5/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6488 - accuracy: 0.5848\n","Epoch 6/32\n","64/64 [==============================] - 0s 5ms/step - loss: 0.6492 - accuracy: 0.5892\n","Epoch 7/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6469 - accuracy: 0.5882\n","Epoch 8/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6455 - accuracy: 0.5975\n","Epoch 9/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6453 - accuracy: 0.5867\n","Epoch 10/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6471 - accuracy: 0.5936\n","Epoch 11/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6447 - accuracy: 0.5926\n","Epoch 12/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6442 - accuracy: 0.5970\n","Epoch 13/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6446 - accuracy: 0.5940\n","Epoch 14/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6432 - accuracy: 0.5970\n","Epoch 15/32\n","64/64 [==============================] - 0s 5ms/step - loss: 0.6442 - accuracy: 0.5931\n","Epoch 16/32\n","64/64 [==============================] - 0s 5ms/step - loss: 0.6421 - accuracy: 0.5975\n","Epoch 17/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6413 - accuracy: 0.5965\n","Epoch 18/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6454 - accuracy: 0.5975\n","Epoch 19/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6433 - accuracy: 0.5989\n","Epoch 20/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6429 - accuracy: 0.5989\n","Epoch 21/32\n","64/64 [==============================] - 0s 5ms/step - loss: 0.6396 - accuracy: 0.6009\n","Epoch 22/32\n","64/64 [==============================] - 0s 5ms/step - loss: 0.6420 - accuracy: 0.5970\n","Epoch 23/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6402 - accuracy: 0.5950\n","Epoch 24/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6387 - accuracy: 0.5984\n","Epoch 25/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6388 - accuracy: 0.6033\n","Epoch 26/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6407 - accuracy: 0.5994\n","Epoch 27/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6393 - accuracy: 0.5989\n","Epoch 28/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6399 - accuracy: 0.5950\n","Epoch 29/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.5979\n","Epoch 30/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.5989\n","Epoch 31/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6389 - accuracy: 0.5984\n","Epoch 32/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6378 - accuracy: 0.5984\n","Epoch 1/32\n","60/60 [==============================] - 1s 2ms/step - loss: 0.6877 - accuracy: 0.5455\n","Epoch 2/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.5593\n","Epoch 3/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.5820\n","Epoch 4/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6469 - accuracy: 0.5852\n","Epoch 5/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6463 - accuracy: 0.5878\n","Epoch 6/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6501 - accuracy: 0.5868\n","Epoch 7/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6490 - accuracy: 0.5947\n","Epoch 8/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.5921\n","Epoch 9/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6454 - accuracy: 0.5926\n","Epoch 10/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6441 - accuracy: 0.5958\n","Epoch 11/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6427 - accuracy: 0.5931\n","Epoch 12/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6467 - accuracy: 0.5931\n","Epoch 13/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.5958\n","Epoch 14/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.6048\n","Epoch 15/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6483 - accuracy: 0.5905\n","Epoch 16/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6469 - accuracy: 0.5958\n","Epoch 17/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6475 - accuracy: 0.5926\n","Epoch 18/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6418 - accuracy: 0.6011\n","Epoch 19/32\n","60/60 [==============================] - 0s 3ms/step - loss: 0.6433 - accuracy: 0.5942\n","Epoch 20/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.5958\n","Epoch 21/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6426 - accuracy: 0.5984\n","Epoch 22/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6413 - accuracy: 0.5968\n","Epoch 23/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6417 - accuracy: 0.5937\n","Epoch 24/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6449 - accuracy: 0.5974\n","Epoch 25/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6418 - accuracy: 0.5963\n","Epoch 26/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.6000\n","Epoch 27/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6425 - accuracy: 0.5968\n","Epoch 28/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6432 - accuracy: 0.5968\n","Epoch 29/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.6016\n","Epoch 30/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.5984\n","Epoch 31/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6418 - accuracy: 0.5899\n","Epoch 32/32\n","60/60 [==============================] - 0s 2ms/step - loss: 0.6407 - accuracy: 0.5963\n","Epoch 1/32\n","54/54 [==============================] - 1s 2ms/step - loss: 0.6865 - accuracy: 0.5681\n","Epoch 2/32\n","54/54 [==============================] - 0s 2ms/step - loss: 0.6680 - accuracy: 0.6157\n","Epoch 3/32\n","54/54 [==============================] - 0s 2ms/step - loss: 0.6314 - accuracy: 0.6272\n","Epoch 4/32\n","54/54 [==============================] - 0s 2ms/step - loss: 0.6173 - accuracy: 0.6267\n","Epoch 5/32\n","54/54 [==============================] - 0s 3ms/step - loss: 0.6056 - accuracy: 0.6365\n","Epoch 6/32\n","54/54 [==============================] - 0s 3ms/step - loss: 0.6065 - accuracy: 0.6267\n","Epoch 7/32\n","54/54 [==============================] - 0s 2ms/step - loss: 0.6016 - accuracy: 0.6417\n","Epoch 8/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.6063 - accuracy: 0.6417\n","Epoch 9/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.6046 - accuracy: 0.6371\n","Epoch 10/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.6038 - accuracy: 0.6388\n","Epoch 11/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.6027 - accuracy: 0.6423\n","Epoch 12/32\n","54/54 [==============================] - 0s 5ms/step - loss: 0.6007 - accuracy: 0.6446\n","Epoch 13/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.6001 - accuracy: 0.6423\n","Epoch 14/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.6007 - accuracy: 0.6400\n","Epoch 15/32\n","54/54 [==============================] - 0s 5ms/step - loss: 0.6026 - accuracy: 0.6400\n","Epoch 16/32\n","54/54 [==============================] - 0s 5ms/step - loss: 0.5977 - accuracy: 0.6441\n","Epoch 17/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5977 - accuracy: 0.6400\n","Epoch 18/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5974 - accuracy: 0.6400\n","Epoch 19/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5958 - accuracy: 0.6383\n","Epoch 20/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5996 - accuracy: 0.6423\n","Epoch 21/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5962 - accuracy: 0.6458\n","Epoch 22/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5960 - accuracy: 0.6446\n","Epoch 23/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5982 - accuracy: 0.6388\n","Epoch 24/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5964 - accuracy: 0.6423\n","Epoch 25/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5959 - accuracy: 0.6417\n","Epoch 26/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5976 - accuracy: 0.6417\n","Epoch 27/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5947 - accuracy: 0.6406\n","Epoch 28/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5951 - accuracy: 0.6441\n","Epoch 29/32\n","54/54 [==============================] - 0s 4ms/step - loss: 0.5944 - accuracy: 0.6400\n","Epoch 30/32\n","54/54 [==============================] - 0s 5ms/step - loss: 0.5937 - accuracy: 0.6423\n","Epoch 31/32\n","54/54 [==============================] - 0s 5ms/step - loss: 0.5929 - accuracy: 0.6446\n","Epoch 32/32\n","54/54 [==============================] - 0s 5ms/step - loss: 0.5919 - accuracy: 0.6417\n","Epoch 1/32\n","65/65 [==============================] - 1s 5ms/step - loss: 0.6923 - accuracy: 0.5085\n","Epoch 2/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5360\n","Epoch 3/32\n","65/65 [==============================] - 0s 5ms/step - loss: 0.6686 - accuracy: 0.5442\n","Epoch 4/32\n","65/65 [==============================] - 0s 5ms/step - loss: 0.6595 - accuracy: 0.5549\n","Epoch 5/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6608 - accuracy: 0.5534\n","Epoch 6/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6585 - accuracy: 0.5626\n","Epoch 7/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6552 - accuracy: 0.5708\n","Epoch 8/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6557 - accuracy: 0.5592\n","Epoch 9/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6544 - accuracy: 0.5631\n","Epoch 10/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6551 - accuracy: 0.5631\n","Epoch 11/32\n","65/65 [==============================] - 0s 3ms/step - loss: 0.6536 - accuracy: 0.5626\n","Epoch 12/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6562 - accuracy: 0.5665\n","Epoch 13/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6522 - accuracy: 0.5679\n","Epoch 14/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.5684\n","Epoch 15/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6537 - accuracy: 0.5727\n","Epoch 16/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6526 - accuracy: 0.5669\n","Epoch 17/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6524 - accuracy: 0.5674\n","Epoch 18/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.5694\n","Epoch 19/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6525 - accuracy: 0.5713\n","Epoch 20/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6515 - accuracy: 0.5694\n","Epoch 21/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.5698\n","Epoch 22/32\n","65/65 [==============================] - 0s 3ms/step - loss: 0.6499 - accuracy: 0.5698\n","Epoch 23/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6523 - accuracy: 0.5684\n","Epoch 24/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.5694\n","Epoch 25/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6487 - accuracy: 0.5698\n","Epoch 26/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.5718\n","Epoch 27/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6505 - accuracy: 0.5723\n","Epoch 28/32\n","65/65 [==============================] - 0s 3ms/step - loss: 0.6488 - accuracy: 0.5747\n","Epoch 29/32\n","65/65 [==============================] - 0s 3ms/step - loss: 0.6504 - accuracy: 0.5747\n","Epoch 30/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6483 - accuracy: 0.5766\n","Epoch 31/32\n","65/65 [==============================] - 0s 2ms/step - loss: 0.6491 - accuracy: 0.5747\n","Epoch 32/32\n","65/65 [==============================] - 0s 3ms/step - loss: 0.6501 - accuracy: 0.5752\n","Epoch 1/32\n","65/65 [==============================] - 1s 4ms/step - loss: 0.6904 - accuracy: 0.5177\n","Epoch 2/32\n","65/65 [==============================] - 0s 5ms/step - loss: 0.6773 - accuracy: 0.5386\n","Epoch 3/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.5503\n","Epoch 4/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5391\n","Epoch 5/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.5498\n","Epoch 6/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.5478\n","Epoch 7/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6576 - accuracy: 0.5488\n","Epoch 8/32\n","65/65 [==============================] - 0s 5ms/step - loss: 0.6574 - accuracy: 0.5561\n","Epoch 9/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6573 - accuracy: 0.5551\n","Epoch 10/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6561 - accuracy: 0.5600\n","Epoch 11/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6559 - accuracy: 0.5580\n","Epoch 12/32\n","65/65 [==============================] - 0s 5ms/step - loss: 0.6546 - accuracy: 0.5648\n","Epoch 13/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6543 - accuracy: 0.5595\n","Epoch 14/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6539 - accuracy: 0.5600\n","Epoch 15/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6552 - accuracy: 0.5644\n","Epoch 16/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6538 - accuracy: 0.5464\n","Epoch 17/32\n","65/65 [==============================] - 0s 5ms/step - loss: 0.6544 - accuracy: 0.5614\n","Epoch 18/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6533 - accuracy: 0.5619\n","Epoch 19/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6537 - accuracy: 0.5546\n","Epoch 20/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6523 - accuracy: 0.5639\n","Epoch 21/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6512 - accuracy: 0.5760\n","Epoch 22/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6530 - accuracy: 0.5556\n","Epoch 23/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6519 - accuracy: 0.5687\n","Epoch 24/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6515 - accuracy: 0.5658\n","Epoch 25/32\n","65/65 [==============================] - 0s 5ms/step - loss: 0.6540 - accuracy: 0.5629\n","Epoch 26/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6522 - accuracy: 0.5697\n","Epoch 27/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6527 - accuracy: 0.5619\n","Epoch 28/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6513 - accuracy: 0.5668\n","Epoch 29/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6502 - accuracy: 0.5663\n","Epoch 30/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6492 - accuracy: 0.5716\n","Epoch 31/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6502 - accuracy: 0.5639\n","Epoch 32/32\n","65/65 [==============================] - 0s 4ms/step - loss: 0.6487 - accuracy: 0.5653\n","Epoch 1/32\n","59/59 [==============================] - 1s 3ms/step - loss: 0.6842 - accuracy: 0.5623\n","Epoch 2/32\n","59/59 [==============================] - 0s 3ms/step - loss: 0.6609 - accuracy: 0.6147\n","Epoch 3/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6331 - accuracy: 0.6036\n","Epoch 4/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6243 - accuracy: 0.6195\n","Epoch 5/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6198 - accuracy: 0.6200\n","Epoch 6/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.6264\n","Epoch 7/32\n","59/59 [==============================] - 0s 3ms/step - loss: 0.6139 - accuracy: 0.6338\n","Epoch 8/32\n","59/59 [==============================] - 0s 3ms/step - loss: 0.6171 - accuracy: 0.6253\n","Epoch 9/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.6275\n","Epoch 10/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6150 - accuracy: 0.6312\n","Epoch 11/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6137 - accuracy: 0.6359\n","Epoch 12/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6147 - accuracy: 0.6301\n","Epoch 13/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6137 - accuracy: 0.6328\n","Epoch 14/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.6343\n","Epoch 15/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6106 - accuracy: 0.6349\n","Epoch 16/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6090 - accuracy: 0.6322\n","Epoch 17/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6103 - accuracy: 0.6317\n","Epoch 18/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6092 - accuracy: 0.6285\n","Epoch 19/32\n","59/59 [==============================] - 0s 3ms/step - loss: 0.6112 - accuracy: 0.6301\n","Epoch 20/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6081 - accuracy: 0.6306\n","Epoch 21/32\n","59/59 [==============================] - 0s 3ms/step - loss: 0.6083 - accuracy: 0.6312\n","Epoch 22/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6074 - accuracy: 0.6301\n","Epoch 23/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6065 - accuracy: 0.6343\n","Epoch 24/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6083 - accuracy: 0.6370\n","Epoch 25/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6053 - accuracy: 0.6380\n","Epoch 26/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6071 - accuracy: 0.6259\n","Epoch 27/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6063 - accuracy: 0.6317\n","Epoch 28/32\n","59/59 [==============================] - 0s 3ms/step - loss: 0.6068 - accuracy: 0.6338\n","Epoch 29/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6050 - accuracy: 0.6349\n","Epoch 30/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6039 - accuracy: 0.6380\n","Epoch 31/32\n","59/59 [==============================] - 0s 3ms/step - loss: 0.6041 - accuracy: 0.6365\n","Epoch 32/32\n","59/59 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.6386\n","Epoch 1/32\n","64/64 [==============================] - 1s 2ms/step - loss: 0.6961 - accuracy: 0.5007\n","Epoch 2/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6869 - accuracy: 0.5366\n","Epoch 3/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6731 - accuracy: 0.5567\n","Epoch 4/32\n","64/64 [==============================] - 0s 3ms/step - loss: 0.6569 - accuracy: 0.5739\n","Epoch 5/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6540 - accuracy: 0.5729\n","Epoch 6/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6534 - accuracy: 0.5729\n","Epoch 7/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.5916\n","Epoch 8/32\n","64/64 [==============================] - 0s 3ms/step - loss: 0.6467 - accuracy: 0.5798\n","Epoch 9/32\n","64/64 [==============================] - 0s 3ms/step - loss: 0.6508 - accuracy: 0.5734\n","Epoch 10/32\n","64/64 [==============================] - 0s 3ms/step - loss: 0.6461 - accuracy: 0.5871\n","Epoch 11/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6456 - accuracy: 0.5876\n","Epoch 12/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.5798\n","Epoch 13/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6477 - accuracy: 0.5783\n","Epoch 14/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6456 - accuracy: 0.5812\n","Epoch 15/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.5857\n","Epoch 16/32\n","64/64 [==============================] - 0s 2ms/step - loss: 0.6431 - accuracy: 0.5783\n","Epoch 17/32\n","64/64 [==============================] - 0s 3ms/step - loss: 0.6423 - accuracy: 0.5783\n","Epoch 18/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6421 - accuracy: 0.5871\n","Epoch 19/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6415 - accuracy: 0.5793\n","Epoch 20/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6430 - accuracy: 0.5822\n","Epoch 21/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6436 - accuracy: 0.5871\n","Epoch 22/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6421 - accuracy: 0.5832\n","Epoch 23/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6427 - accuracy: 0.5812\n","Epoch 24/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6385 - accuracy: 0.5871\n","Epoch 25/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6425 - accuracy: 0.5852\n","Epoch 26/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6422 - accuracy: 0.5886\n","Epoch 27/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6438 - accuracy: 0.5886\n","Epoch 28/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6392 - accuracy: 0.5866\n","Epoch 29/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6398 - accuracy: 0.5842\n","Epoch 30/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6416 - accuracy: 0.5876\n","Epoch 31/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6409 - accuracy: 0.5852\n","Epoch 32/32\n","64/64 [==============================] - 0s 4ms/step - loss: 0.6404 - accuracy: 0.5842\n"]}]},{"cell_type":"code","source":["target_data = cifar_train_fed_data[0]\n","attacker_data = cifar_test_fed_data[0]"],"metadata":{"id":"KAUOAUE0Ccjo","executionInfo":{"status":"ok","timestamp":1676597902456,"user_tz":300,"elapsed":292,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["#attacker_data = cifar_train_data"],"metadata":{"id":"qZ4SCSPsCeqd","executionInfo":{"status":"ok","timestamp":1676597935858,"user_tz":300,"elapsed":261,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["attack_test_data, real_membership_labels = prepare_attack_data(target_model, target_data, attacker_data)"],"metadata":{"id":"f0S_nc2VChT5","executionInfo":{"status":"ok","timestamp":1676597938021,"user_tz":300,"elapsed":622,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":["def results(attack_guesses,real_membership_labels):\n","    pred_labels=attack_guesses\n","    true_labels=real_membership_labels\n","\n","    # True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\n","    TP = np.sum(np.logical_and(pred_labels == 1, true_labels == 1))\n","\n","    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n","    TN = np.sum(np.logical_and(pred_labels == 0, true_labels == 0))\n","\n","    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n","    FP = np.sum(np.logical_and(pred_labels == 1, true_labels == 0))\n","\n","    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n","    FN = np.sum(np.logical_and(pred_labels == 0, true_labels == 1))\n","\n","    print ('TP: %i, FP: %i, TN: %i, FN: %i' % (TP,FP,TN,FN))\n","\n","    print(\"acc = \" + str((TP+TN)/(TP+TN+FP+FN)))\n","    print(\"precision = \" + str((TP)/(TP+FP)))\n","    print(\"recall = \" + str((TP)/(TP+FN)))\n","    \n","    acc= (TP+TN)/(TP+TN+FP+FN)\n","    prec=(TP)/(TP+FP)\n","    rec=(TP)/(TP+FN)\n","    return [acc,prec,rec]"],"metadata":{"id":"QempKD6FCjJ3","executionInfo":{"status":"ok","timestamp":1676597939036,"user_tz":300,"elapsed":9,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":["attack_guesses = amb.predict(attack_test_data)\n","attack_precision = np.mean((attack_guesses == 1) == (real_membership_labels == 1))\n","\n","class_precision = []\n","\n","for c in range(NUM_CLASSES):\n","    #attack_test_data, real_membership_labels = prepare_attack_data(centralized_model, cifar_train_data, attacker_data)\n","    target_indices = [i for i, d in enumerate(target_data[1].argmax(axis=1)) if d == c]\n","    test_indices = [i for i, d in enumerate(attacker_data[1].argmax(axis=1)) if d == c]\n","\n","\n","    print(np.sum(attack_guesses[target_indices]==1) / (np.sum(attack_guesses[target_indices]) + np.sum(attack_guesses[SIZE:][test_indices])))\n","    \n","    class_precision.append(\n","            np.sum(attack_guesses[target_indices]==1) / (np.sum(attack_guesses[target_indices])\n","                                                     + np.sum(attack_guesses[SIZE:][test_indices])))\n","print(\"Average Accuracy: \", attack_precision)\n","\n","result=results(attack_guesses,real_membership_labels)\n"," \n","    #attack_accuracy_class[c].append(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZ5mAp0RCj9u","executionInfo":{"status":"ok","timestamp":1676597942370,"user_tz":300,"elapsed":1241,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}},"outputId":"f169efc8-959c-468a-d782-b5a0a48e1c29"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["0.26479750778816197\n","0.27323943661971833\n","0.2434017595307918\n","0.24085365853658536\n","0.25274725274725274\n","0.250814332247557\n","0.22\n","0.25745257452574527\n","0.2465277777777778\n","0.25396825396825395\n","Average Accuracy:  0.3335\n","TP: 836, FP: 2502, TN: 498, FN: 164\n","acc = 0.3335\n","precision = 0.2504493708807669\n","recall = 0.836\n"]}]},{"cell_type":"markdown","source":["**MIA via Prediction Sensitivity**"],"metadata":{"id":"kbKDEsoGCn6_"}},{"cell_type":"code","execution_count":102,"metadata":{"id":"Ujiv0TziESPQ","executionInfo":{"status":"ok","timestamp":1676597957127,"user_tz":300,"elapsed":268,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["import pickle\n","import argparse\n","\n","import numpy as np\n","import torch\n","from numpy import linalg as LA\n","from sklearn.metrics import precision_score, recall_score\n","from sklearn.cluster import SpectralClustering"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1676597958270,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"},"user_tz":300},"id":"anKyVFt4H8LP","outputId":"92cec328-e8c4-4785-875c-da7a36536f4f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fe07a5e96d0>"]},"metadata":{},"execution_count":103}],"source":["np.random.seed(seed=14)\n","torch.manual_seed(14)"]},{"cell_type":"code","execution_count":104,"metadata":{"id":"uR6nQL2dH_DH","executionInfo":{"status":"ok","timestamp":1676597960290,"user_tz":300,"elapsed":422,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["target_model = student_model\n","#target_model = single_model0"]},{"cell_type":"code","execution_count":105,"metadata":{"id":"8ao_HeM-IO-c","executionInfo":{"status":"ok","timestamp":1676597961192,"user_tz":300,"elapsed":6,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--n_sample', type=int, default=5000)\n","parser.add_argument('--n_attack', type=int, default=50)\n","parser.add_argument('--seed', type=int, default=140)\n","parser.add_argument('--neighbors', type=int, default=40)\n","parser.add_argument('--data_generate', type=bool, default=False)\n","attack_args = parser.parse_args(args=[])"]},{"cell_type":"code","execution_count":106,"metadata":{"id":"A73f1_7gJcfT","executionInfo":{"status":"ok","timestamp":1676597963169,"user_tz":300,"elapsed":285,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["precisions = []\n","recalls = []\n","f1_scores = []"]},{"cell_type":"code","execution_count":107,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1676597964299,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"},"user_tz":300},"id":"TNUiUWYZJfCX","outputId":"5dc343e0-d944-4a6b-898f-549592ba79ab"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fe07a5e96d0>"]},"metadata":{},"execution_count":107}],"source":["np.random.seed(seed=attack_args.seed)\n","torch.manual_seed(attack_args.seed)"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"JwX1ZwpJLqi7","executionInfo":{"status":"ok","timestamp":1676597965844,"user_tz":300,"elapsed":4,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import numpy as np\n","import sklearn\n","from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from collections import Counter\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":172575,"status":"ok","timestamp":1676595351779,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"},"user_tz":300},"id":"ty6LlBBURROc","outputId":"2d608232-60dd-4a65-e9d9-3de4b44292d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Proposal work\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/My Drive/Proposal work'"]},{"cell_type":"code","execution_count":109,"metadata":{"id":"WU1H99mQJjIC","executionInfo":{"status":"ok","timestamp":1676597967941,"user_tz":300,"elapsed":273,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["def data_reader(data_name = \"mnist\"):\n","    file_path = \"data/\"\n","    #data = pd.read_csv(file_path + 'mnist_train.csv', header=1)\n","    data = pd.read_csv(file_path + 'mnist_train.csv', header=1, skiprows=30000, nrows=29999)\n","    #data = pd.read_csv(file_path + 'fashion-mnist_train.csv', header=1)\n","    #data = pd.read_csv(file_path + 'fashion-mnist_train.csv', header=1, skiprows=30000, nrows=29999)\n","    #data = pd.read_csv(file_path + 'fashion-mnist_test.csv', header=0)\n","    #data = pd.read_csv(file_path + 'cifar_train.csv', header=1, skiprows=30000, nrows=29999)\n","    data = np.array(data)\n","    labels = data[:,0]\n","    data = data[:,1:]\n","        \n","    categorical_features = []\n","    \n","    data = data/data.max()\n","    oh_encoder = ColumnTransformer(\n","    [('oh_enc', OneHotEncoder(sparse=False), categorical_features),], \n","    remainder='passthrough' )\n","    oh_data = oh_encoder.fit_transform(data)\n","        \n","    #randomly select 10000 records as training data\n","    train_idx = np.random.choice(len(labels), 9999, replace = False)\n","    idx = range(len(labels))\n","    idx = np.array(idx)\n","    test_idx = list(set(idx).difference(set(train_idx)))\n","    test_idx = np.array(test_idx)\n","    \n","    assert test_idx.sum() + train_idx.sum() == idx.sum()\n","    \n","    X_train = data[train_idx,:]\n","    Y_train = labels[train_idx]\n","    \n","    X_test = data[test_idx,:]\n","    Y_test = labels[test_idx]\n","    \n","    orig_dataset = {\"X_train\":X_train,\n","               \"Y_train\":Y_train,\n","               \"X_test\":X_test,\n","               \"Y_test\":Y_test}\n","    \n","    X_train = oh_data[train_idx,:]\n","    \n","    X_test = oh_data[test_idx,:]\n","    \n","    oh_dataset = {\"X_train\":X_train,\n","               \"Y_train\":Y_train,\n","               \"X_test\":X_test,\n","               \"Y_test\":Y_test}\n","\n","    return orig_dataset, oh_dataset, oh_encoder"]},{"cell_type":"code","execution_count":110,"metadata":{"id":"oUwJJKEeLzy5","executionInfo":{"status":"ok","timestamp":1676597977750,"user_tz":300,"elapsed":6048,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["orig_dataset, oh_dataset, OH_Encoder = data_reader(\"mnist\")"]},{"cell_type":"code","execution_count":111,"metadata":{"id":"6QNtvok1_LUF","executionInfo":{"status":"ok","timestamp":1676597977752,"user_tz":300,"elapsed":7,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["class_label_for_count = np.unique(np.hstack([orig_dataset[\"Y_train\"], orig_dataset[\"Y_test\"]]))\n","n_class = len(class_label_for_count)\n","n_features = orig_dataset['X_train'].shape[1]"]},{"cell_type":"code","execution_count":112,"metadata":{"id":"ioKqcQ3K_fZP","executionInfo":{"status":"ok","timestamp":1676597980827,"user_tz":300,"elapsed":301,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["y_attack = np.hstack(([np.ones(int(attack_args.n_attack/2)), np.zeros(int(attack_args.n_attack/2))]))\n","x_attack = np.zeros((int(attack_args.n_attack), n_features))"]},{"cell_type":"code","execution_count":113,"metadata":{"id":"27B9EGTv_i8M","executionInfo":{"status":"ok","timestamp":1676597982872,"user_tz":300,"elapsed":286,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["Jacobian_matrix = np.zeros([attack_args.n_attack, n_class, n_features])"]},{"cell_type":"code","execution_count":114,"metadata":{"id":"2VkIP4RD_pcA","executionInfo":{"status":"ok","timestamp":1676597984103,"user_tz":300,"elapsed":3,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["if attack_args.data_generate:\n","    output_x = np.zeros((attack_args.n_attack, n_features))\n","    output_y = y_attack\n","    classes = np.zeros((attack_args.n_attack, 1))"]},{"cell_type":"code","execution_count":115,"metadata":{"id":"sJPuhlKz_961","executionInfo":{"status":"ok","timestamp":1676597985881,"user_tz":300,"elapsed":396,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["def fn_R_given_Selected(dataset, IN_or_OUT = 1):\n","    if(IN_or_OUT == 1):#IN_or_OUT == 1 meaning selecting R_given from training set\n","        idx = np.random.choice( len(dataset['Y_train']) )\n","        R_given = dataset['X_train'][idx,:]\n","        R_given_y = dataset['Y_train'][idx]\n","    elif(IN_or_OUT == 0):#IN_or_OUT == 0 meaning selecting R_given from testing set\n","        idx = np.random.choice( len(dataset['Y_test']) )\n","        R_given = dataset['X_test'][idx,:]\n","        R_given_y = dataset['Y_test'][idx]\n","    return R_given, R_given_y"]},{"cell_type":"code","execution_count":116,"metadata":{"id":"Ew93UQtdAqWS","executionInfo":{"status":"ok","timestamp":1676597987656,"user_tz":300,"elapsed":267,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["def Target_Model_pred_fn(Target_Model, X_test):\n","    pred_proba = Target_Model.predict_proba(X_test)\n","    return pred_proba"]},{"cell_type":"code","execution_count":117,"metadata":{"id":"pop4W0YrAPef","executionInfo":{"status":"ok","timestamp":1676597988898,"user_tz":300,"elapsed":8,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["categorical_list ={\n","    \"mnist\": [1,2,3,4,5,6,7,8,9,10],\n","}"]},{"cell_type":"code","execution_count":118,"metadata":{"id":"U-xTRmmcADLZ","executionInfo":{"status":"ok","timestamp":1676597990435,"user_tz":300,"elapsed":273,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["def fn_Sample_Generator(R_given, dataset):\n","    if not dataset in categorical_list.keys():\n","        dataset = \"null\"\n","    epsilon = 1e-6\n","    R_given = R_given.reshape([1, -1])\n","    n_feature = R_given.shape[1]\n","    local_samples = np.repeat(R_given, repeats=n_feature, axis=0)\n","    for i in range(n_feature):\n","        if i in categorical_list[dataset]:\n","            continue\n","        local_samples[i][i] += epsilon\n","\n","    return local_samples"]},{"cell_type":"code","execution_count":119,"metadata":{"id":"2e-Y9y2gA3r3","executionInfo":{"status":"ok","timestamp":1676597994552,"user_tz":300,"elapsed":348,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["def fn_Jacobian_Calculation(R_given, local_proba, n_features, n_class):\n","    epsilon = 1e-6\n","    jacobian = np.zeros([n_class, n_features])\n","\n","    for ii in range(n_class):\n","        jacobian[ii, :] = (local_proba[:, ii] - R_given[ii]) / epsilon\n","    return jacobian"]},{"cell_type":"code","execution_count":120,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14506,"status":"ok","timestamp":1676598010549,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"},"user_tz":300},"id":"NaYgLSL7_tjW","outputId":"81473f10-0581-441a-b80e-c5c293258275"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_78_input'), name='flatten_78_input', description=\"created by layer 'flatten_78_input'\"), but it was called on an input with incompatible shape (None, 784).\n"]}],"source":["for ii in range(attack_args.n_attack):\n","      R_x, R_y = fn_R_given_Selected(orig_dataset, IN_or_OUT=y_attack[ii])\n","      R_x_OH = OH_Encoder.transform(R_x.reshape(1, -1))\n","      x_attack[ii] = R_x\n","      local_samples = fn_Sample_Generator(R_x, \"mnist\")\n","      oh_local_samples = OH_Encoder.transform(local_samples)\n","      local_proba = Target_Model_pred_fn(target_model, oh_local_samples)\n","      R_local_proba = Target_Model_pred_fn(target_model, R_x_OH)\n","      Jacobian_matrix[ii] = fn_Jacobian_Calculation(R_local_proba[0], local_proba, n_features, n_class)\n","\n","      if attack_args.data_generate:\n","          output_x[ii] = R_x\n","          classes[ii] = R_y"]},{"cell_type":"code","execution_count":122,"metadata":{"id":"hMNyqVZEBmVr","executionInfo":{"status":"ok","timestamp":1676598015410,"user_tz":300,"elapsed":279,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["Jacobian_norms = LA.norm(Jacobian_matrix, axis=(1, 2))"]},{"cell_type":"code","execution_count":123,"metadata":{"id":"sF1ghDXZBpkt","executionInfo":{"status":"ok","timestamp":1676598016914,"user_tz":300,"elapsed":395,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"}}},"outputs":[],"source":["split = 1\n","attack_cluster = SpectralClustering(n_clusters=6, n_jobs=-1, affinity='nearest_neighbors', n_neighbors=19)\n","y_attack_pred = attack_cluster.fit_predict(Jacobian_norms.reshape(-1, 1))\n","cluster_1 = np.where(y_attack_pred >= split)[0]\n","cluster_0 = np.where(y_attack_pred < split)[0]\n","y_attack_pred[cluster_1] = 1\n","y_attack_pred[cluster_0] = 0\n","cluster_1_mean_norm = Jacobian_norms[cluster_1].mean()\n","cluster_0_mean_norm = Jacobian_norms[cluster_0].mean()\n","if cluster_1_mean_norm > cluster_0_mean_norm:\n","  y_attack_pred = np.abs(y_attack_pred-1)"]},{"cell_type":"code","execution_count":124,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":464,"status":"ok","timestamp":1676598018952,"user":{"displayName":"Saroj Dayal","userId":"17554496406606940001"},"user_tz":300},"id":"8_Uk38OPB9Eb","outputId":"2db662e9-d166-45a9-bb9e-2eabb4e2128f"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.42105263157894735 0.32 0.3636363636363636\n"]}],"source":["precision = precision_score(y_attack, y_attack_pred)\n","recall = recall_score(y_attack, y_attack_pred)\n","f1_score = 2*precision*recall/(precision+recall)\n","print(precision, recall, f1_score)\n","precisions.append(precision)\n","recalls.append(recall)\n","f1_scores.append(f1_score)"]}]}